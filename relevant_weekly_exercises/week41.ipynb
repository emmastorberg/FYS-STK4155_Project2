{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly exercise, week 41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we did not have time to implement all of this week's tasks, as we chose to spend a while setting up generalizable code infrastructure that can be used as is in our final project. We hope you are okay with this prioritization, and have made a plan to finish this  over the weekend. Please let us know if we should submit the full code for correction at a later time.\n",
    "\n",
    "We have done the three first tasks. Our plan for implementing learning rate methods is to include the cost function (with regularization if specified) in the base class, and then have a method of if-tests that handles eta if learning rate tuning is specified. We plan on calling this method from the `perform` method. \n",
    "\n",
    "We also struggle with the regularization term, so that we can perform the same analysis for Ridge. We think it's a quick fix, but have not prioritized it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GD(ABC):\n",
    "    def __init__(\n",
    "            self,\n",
    "            eta: float,\n",
    "            delta_momentum: float | None,\n",
    "            max_iter: int, \n",
    "            tol: float, \n",
    "            rng: np.random.Generator | None,\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize base class for gradient descent methods.\n",
    "\n",
    "        Args:\n",
    "            max_iter (int): maximum number of iterations before termination.\n",
    "            tol (int): terminate when cost is below `tol`.\n",
    "            rng (np.random.Generator or None): random generator. If None, rng.random.default_rng(None) is used.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        # self.eta_tuner = eta_tuner        # TODO: implement learning rate tuners\n",
    "        self.delta_momentum = delta_momentum\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        if rng is None:\n",
    "            rng = np.random.default_rng(None)\n",
    "        self.rng = rng\n",
    "\n",
    "        # if eta_tuner is None:\n",
    "        #     self.tune = False\n",
    "        # else:\n",
    "        #     if not (eta_tuner in [\"adagrad, rmsprop, adam\"]):\n",
    "        #         raise ValueError\n",
    "        #     self.tune = True\n",
    "\n",
    "        if delta_momentum is None:\n",
    "            self.momentum = False\n",
    "        else:\n",
    "            self.momentum = True\n",
    "\n",
    "        self.gradient = None\n",
    "        self.cost = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.lmbda = 0\n",
    "        self.X_num_rows = None\n",
    "        self.X_num_cols = None\n",
    "    \n",
    "    def set_cost(self):\n",
    "        \"\"\"\n",
    "        Not yet implemented correctly. Will be used when learning rate tuning is implemented.\n",
    "        \"\"\"\n",
    "        self.cost = lambda X, y, beta: (1/self.X_num_rows) * np.sum((y - (X @ beta))**2)\n",
    "        # TODO: Fix this\n",
    "        # if self.lmbda:\n",
    "        #     self.cost = lambda X, y, beta: self.cost(X, y, beta) + self.lmbda * np.sum(beta**2)\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_gradient(self, X: np.ndarray, y: np.ndarray, lmbda: float | int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Setter method??\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.lmbda = lmbda\n",
    "        self.X_num_rows, self.X_num_cols = X.shape\n",
    "        self.set_cost()\n",
    "        # raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def perform(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def add_momentum(self, delta, delta_0):\n",
    "        delta += self.delta_momentum * delta_0\n",
    "        delta_0 = delta\n",
    "        return delta, delta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainGD(GD):\n",
    "    def __init__(\n",
    "            self, \n",
    "            eta: float = 0.01,\n",
    "            eta_tuner: str | None = None,\n",
    "            delta_momentum: float | None = None,\n",
    "            max_iter: int = 50, \n",
    "            tol: float = 1e-8, \n",
    "            rng: np.random.Generator | None = None,\n",
    "        ) -> None:\n",
    "        super().__init__(eta, delta_momentum, max_iter, tol, rng)\n",
    "\n",
    "    def set_gradient(self, X: np.ndarray, y: np.ndarray, lmbda: float | int = 0) -> None:\n",
    "        super().set_gradient(X, y, lmbda)\n",
    "        self.gradient = lambda beta: (2.0/self.X_num_rows) * X.T @ (X @ beta - y)\n",
    "        # TODO: FIX THIS\n",
    "        # if lmbda:\n",
    "        #     gradient = lambda beta: self.gradient(beta) + 2*lmbda*beta\n",
    "        #     self.gradient = gradient\n",
    "\n",
    "    def perform(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Performs the descent iteratively.\n",
    "\n",
    "        Args:\n",
    "            Tol (float): when to terminate.\n",
    "\n",
    "        Returns:\n",
    "            (np.ndarray): beta.\n",
    "        \"\"\"\n",
    "        cost = 10 # TODO: change this to the actual cost\n",
    "        beta = self.rng.random(self.X_num_cols)\n",
    "        i = 0\n",
    "        delta_0 = 0.0\n",
    "        while (cost > self.tol) and (i < self.max_iter):\n",
    "            delta = self.eta * self.gradient(beta)\n",
    "            if self.momentum:\n",
    "                delta, delta_0 = self.add_momentum(delta, delta_0)\n",
    "            beta -= delta\n",
    "            i += 1\n",
    "        return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGD(GD):\n",
    "    def __init__(\n",
    "            self, \n",
    "            eta: float = 0.01,\n",
    "            delta_momentum: float | None = None,\n",
    "            max_iter: int = 50,\n",
    "            tol: float = 10e-8, \n",
    "            rng: np.random.Generator | None = None, \n",
    "            M: int = 5, \n",
    "            num_epochs: int = 50, \n",
    "            t0: int = 5, \n",
    "            t1: int = 50\n",
    "        ) -> None:\n",
    "        super().__init__(eta, delta_momentum, max_iter, tol, rng)\n",
    "\n",
    "        self.M = M\n",
    "        self.num_epochs = num_epochs\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "\n",
    "    def set_gradient(self, X: np.ndarray, y: np.ndarray,  lmbda: float | int = 0) -> None:\n",
    "        super().set_gradient(X, y, lmbda)\n",
    "        self.gradient = lambda beta, xi, yi: (2.0/self.X_num_rows) * xi.T @ (xi @ beta - yi)\n",
    "        # TODO: FIX THIS\n",
    "        # if lmbda:\n",
    "        #     gradient = lambda beta, xi, yi: self.gradient(beta, xi, yi) + 2*lmbda*beta\n",
    "        #     self.gradient = gradient\n",
    "\n",
    "    def learning_schedule(self, t: int) -> float:\n",
    "        return self.t0/(t+self.t1)\n",
    "    \n",
    "    def perform(self) -> np.ndarray:\n",
    "        m = int(self.X_num_cols/self.M)\n",
    "        beta = self.rng.random(self.X_num_cols)\n",
    "        delta_0 = 0.0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            m_range = np.arange(0, m - 1)\n",
    "            self.rng.shuffle(m_range)\n",
    "            for k in m_range:\n",
    "                xk = self.X[k:k+self.M]\n",
    "                yk = self.y[k:k+self.M]\n",
    "                eta = self.learning_schedule(epoch*m + k)\n",
    "                delta = eta*self.gradient(beta, xk, yk)\n",
    "                if self.momentum:\n",
    "                    delta, delta_0 = self.add_momentum(delta, delta_0)\n",
    "                beta -= delta\n",
    "        return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "rng = np.random.default_rng(10)\n",
    "x = np.linspace(0, 1, n)\n",
    "y = 2*x\n",
    "X = np.c_[np.ones(n), x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD = PlainGD(eta=0.2)\n",
    "GD.set_gradient(X, y)\n",
    "beta = GD.perform()\n",
    "\n",
    "GDM = PlainGD(eta=0.2, delta_momentum=0.3)\n",
    "GDM.set_gradient(X, y)\n",
    "betam = GDM.perform()\n",
    "\n",
    "SGD = StochasticGD(eta=0.2, t0=1, t1=10)\n",
    "SGD.set_gradient(X, y)\n",
    "betasgd = SGD.perform()\n",
    "\n",
    "SGDM = StochasticGD(eta=0.02, delta_momentum=0.3, t0=0.1, t1=1)\n",
    "SGDM.set_gradient(X, y)\n",
    "betasgdm = SGDM.perform()\n",
    "\n",
    "beta_linreg = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "\n",
    "xnew = np.array([[0],[2]])\n",
    "xbnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredictsgd = xbnew.dot(betasgd)\n",
    "ypredictm = xbnew.dot(betam)\n",
    "ypredict = xbnew.dot(beta)\n",
    "ypredictsgdm = xbnew.dot(betasgdm)\n",
    "ypredict2 = xbnew.dot(beta_linreg)\n",
    "plt.plot(xnew, ypredictsgdm, label=\"sgdm\")\n",
    "plt.plot(xnew, ypredictsgd, label = \"sgd\")\n",
    "plt.plot(xnew, ypredictm, label=\"gdm\")\n",
    "plt.plot(xnew, ypredict, \"r-\", label=\"gd\")\n",
    "plt.plot(xnew, ypredict2, \"b-\", label=\"analytical\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Gradient descent vs Analytical, OLS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the gradient descent methods does not perform as well as the analytical. This makes sence since the anaytical is exact, and gradient descent is a numerical approach. We think that if we had spent some more time on tuning the parameters, the results would be better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fysstk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
