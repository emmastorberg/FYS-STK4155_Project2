{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1654,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np  # We need to use this numpy wrapper to make automatic differentiation work later\n",
    "from autograd import grad, elementwise_grad\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Defining some activation functions\n",
    "def ReLU(z):\n",
    "    return np.where(z > 0, z, 0)\n",
    "\n",
    "# Derivative of the ReLU function\n",
    "def ReLU_der(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def mse(predict, target):\n",
    "    return np.mean((predict - target) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2a)\n",
    "\n",
    "The shape of weights and biases will be determined based on the input and output sizes of the network. In this case, the input size will be 2, and the output size will be 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1655,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2b)\n",
    "def feed_forward_one_layer(W, b, x):\n",
    "    z = W @ x + b\n",
    "    a = sigmoid(z)\n",
    "    return a\n",
    "\n",
    "def cost_one_layer(W, b, x, target):\n",
    "    predict = feed_forward_one_layer(W, b, x)\n",
    "    return mse(predict, target)\n",
    "\n",
    "\n",
    "x = np.random.rand(2)\n",
    "target = np.random.rand(3)\n",
    "\n",
    "W = np.random.randn(len(target), len(x))\n",
    "b = np.random.randn(len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0806643  -0.04842969]\n",
      " [-0.0281288  -0.01688813]\n",
      " [-0.04492729 -0.0269737 ]] [-0.08157134 -0.0284451  -0.04543248]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2c)\n",
    "autograd_one_layer = grad(cost_one_layer, [0, 1])\n",
    "W_g, b_g = autograd_one_layer(W, b, x, target)\n",
    "print(W_g, b_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3a)\n",
    "\n",
    "The reusable results are dC/da and da/dz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3264923  -0.13291599 -0.49424357]\n",
      "[-0.3264923  -0.13291599 -0.49424357]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3b)\n",
    "z = W @ x + b\n",
    "a = sigmoid(z)\n",
    "\n",
    "predict = a\n",
    "\n",
    "def mse_der(predict, target):\n",
    "    return 2/len(predict) * (predict - target)\n",
    "\n",
    "print(mse_der(predict, target))\n",
    "\n",
    "cost_autograd = grad(mse, 0)\n",
    "print(cost_autograd(predict, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24984155 0.2140081  0.09192326]\n",
      "[0.24984155 0.2140081  0.09192326]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3c)\n",
    "def sigmoid_der(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "print(sigmoid_der(z))\n",
    "\n",
    "sigmoid_autograd = elementwise_grad(sigmoid, 0)\n",
    "print(sigmoid_autograd(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) (3,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3d) \n",
    "dC_da = mse_der(a, target)\n",
    "dC_dz = dC_da * sigmoid_der(z)\n",
    "\n",
    "print(dC_da.shape, dC_dz.shape)\n",
    "print(sigmoid_der(z).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3e)\n",
    "dz_dW = np.tensordot(np.eye(len(target)), x, axes=0)\n",
    "dz_db = np.ones(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0806643  -0.04842969]\n",
      " [-0.0281288  -0.01688813]\n",
      " [-0.04492729 -0.0269737 ]] [-0.08157134 -0.0284451  -0.04543248]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3f)\n",
    "dC_da = mse_der(a, target)\n",
    "dC_dz = dC_da * sigmoid_der(z)\n",
    "dC_dW = dC_dz @ dz_dW\n",
    "dC_db = dC_dz * dz_db\n",
    "\n",
    "print(dC_dW, dC_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0806643  -0.04842969]\n",
      " [-0.0281288  -0.01688813]\n",
      " [-0.04492729 -0.0269737 ]] [-0.08157134 -0.0284451  -0.04543248]\n"
     ]
    }
   ],
   "source": [
    "W_g, b_g = autograd_one_layer(W, b, x, target)\n",
    "print(W_g, b_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1663,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(2)\n",
    "target = np.random.rand(4)\n",
    "\n",
    "W1 = np.random.rand(3, 2)\n",
    "b1 = np.random.rand(3)\n",
    "\n",
    "W2 = np.random.rand(4, 3)\n",
    "b2 = np.random.rand(4)\n",
    "\n",
    "layers = [(W1, b1), (W2, b2)]\n",
    "\n",
    "z1 = W1 @ x + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = W2 @ a1 + b2\n",
    "a2 = sigmoid(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1664,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4a)\n",
    "dC_da2 = mse_der(a2, target) # OK\n",
    "dC_dz2 = dC_da2 * sigmoid_der(z2) # check vector as exponent\n",
    "dC_dW2 = dC_dz2 @ np.tensordot(np.eye(len(z2)), a1, axes=0)\n",
    "dC_db2 = dC_dz2 # deriv wrt b2 is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4b)\n",
    "\n",
    "The derivative of the second layer intermediate z2 wrt. the first layer activation a1 is a row vector where each entry is the sum of the corresponding row in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00412035 0.00806305]\n",
      " [0.00421007 0.00823863]\n",
      " [0.00358914 0.00702353]] [0.00812245 0.00829932 0.00707527]\n",
      "[[-0.01723865 -0.02157461 -0.02236588]\n",
      " [ 0.03704805  0.04636658  0.04806713]\n",
      " [ 0.02956084  0.03699614  0.03835302]\n",
      " [ 0.00109094  0.00136534  0.00141542]] [-0.02825852  0.06073115  0.04845771  0.00178833]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4c)\n",
    "dC_da1 = dC_dz2 @ W2 # OK\n",
    "dC_dz1 = dC_da1 * sigmoid_der(z1) # check vector as exponent\n",
    "dC_dW1 = dC_dz1 @ np.tensordot(np.eye(len(z1)), x, axes=0) # OK\n",
    "dC_db1 = dC_dz1 # deriv wrt b1 is 1\n",
    "\n",
    "print(dC_dW1, dC_db1)\n",
    "print(dC_dW2, dC_db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[0.00412035, 0.00806305],\n",
       "         [0.00421007, 0.00823863],\n",
       "         [0.00358914, 0.00702353]]),\n",
       "  array([0.00812245, 0.00829932, 0.00707527])),\n",
       " (array([[-0.01723865, -0.02157461, -0.02236588],\n",
       "         [ 0.03704805,  0.04636658,  0.04806713],\n",
       "         [ 0.02956084,  0.03699614,  0.03835302],\n",
       "         [ 0.00109094,  0.00136534,  0.00141542]]),\n",
       "  array([-0.02825852,  0.06073115,  0.04845771,  0.00178833]))]"
      ]
     },
     "execution_count": 1666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 4d)\n",
    "def feed_forward_two_layers(layers, x):\n",
    "    W1, b1 = layers[0]\n",
    "    z1 = W1 @ x + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    W2, b2 = layers[1]\n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    return a2\n",
    "\n",
    "def cost_two_layers(layers, x, target):\n",
    "    predict = feed_forward_two_layers(layers, x)\n",
    "    return mse(predict, target)\n",
    "\n",
    "\n",
    "grad_two_layers = grad(cost_two_layers, 0)\n",
    "grad_two_layers(layers, x, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4e)\n",
    "The first derivative (the cost function) will be used one time on the outer layer. On the layer in question, we differentiate wrt W or b, but for intermediate layers we differentiate the activation functions and application of weight and bias over and over, until we reach the layer we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1667,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layers(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "\n",
    "    i_size = network_input_size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        W = np.random.randn(layer_output_size, i_size)\n",
    "        b = np.random.randn(layer_output_size)\n",
    "        layers.append((W, b))\n",
    "\n",
    "        i_size = layer_output_size\n",
    "    return layers\n",
    "\n",
    "\n",
    "def feed_forward(input, layers, activation_funcs):\n",
    "    a = input\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = W @ a + b\n",
    "        a = activation_func(z)\n",
    "    return a\n",
    "\n",
    "def cost(layers, input, activation_funcs, target):\n",
    "    predict = feed_forward(input, layers, activation_funcs)\n",
    "    return mse(predict, target)\n",
    "\n",
    "def feed_forward_saver(input, layers, activation_funcs):\n",
    "    layer_inputs = []\n",
    "    zs = []\n",
    "    a = input\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        layer_inputs.append(a)\n",
    "        z = W @ a + b\n",
    "        a = activation_func(z)\n",
    "\n",
    "        zs.append(z)\n",
    "\n",
    "    return layer_inputs, zs, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1668,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5a)\n",
    "def backpropagation(\n",
    "    input, layers, activation_funcs, target, activation_ders, cost_der=mse_der\n",
    "):\n",
    "    layer_inputs, zs, predict = feed_forward_saver(input, layers, activation_funcs)\n",
    "\n",
    "    layer_grads = [() for layer in layers]\n",
    "\n",
    "    # We loop over the layers, from the last to the first\n",
    "    for i in reversed(range(len(layers))):\n",
    "        layer_input, z, activation_der = layer_inputs[i], zs[i], activation_ders[i]\n",
    "\n",
    "        if i == len(layers) - 1:\n",
    "            # For last layer we use cost derivative as dC_da(L) can be computed directly\n",
    "            dC_da = cost_der(predict, target)\n",
    "        else:\n",
    "            # For other layers we build on previous z derivative, as dC_da(i) = dC_dz(i+1) * dz(i+1)_da(i)\n",
    "            (W, b) = layers[i + 1]\n",
    "            dC_da = dC_dz @ W\n",
    "\n",
    "        dC_dz = dC_da * activation_der(z)\n",
    "        dC_dW = dC_dz @ np.tensordot(np.eye(len(z)), layer_input, axes=0)\n",
    "        dC_db = dC_dz # deriv wrt b is 1\n",
    "\n",
    "        layer_grads[i] = (dC_dW, dC_db)\n",
    "\n",
    "    return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[ 0.0040953 ,  0.04639799],\n",
      "       [ 0.05674624,  0.64290998],\n",
      "       [-0.00898892, -0.10184046]]), array([ 0.05622079,  0.77901893, -0.12340087])), (array([[0.00175181, 0.00429662, 0.00061569],\n",
      "       [0.20018365, 0.49098549, 0.07035625],\n",
      "       [0.06733028, 0.16513931, 0.0236638 ],\n",
      "       [0.1903718 , 0.46692022, 0.06690779]]), array([0.00660111, 0.75432519, 0.25371165, 0.71735252]))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[ 0.0040953 ,  0.04639799],\n",
       "         [ 0.05674624,  0.64290998],\n",
       "         [-0.00898892, -0.10184046]]),\n",
       "  array([ 0.05622079,  0.77901893, -0.12340087])),\n",
       " (array([[0.00175181, 0.00429662, 0.00061569],\n",
       "         [0.20018365, 0.49098549, 0.07035625],\n",
       "         [0.06733028, 0.16513931, 0.0236638 ],\n",
       "         [0.1903718 , 0.46692022, 0.06690779]]),\n",
       "  array([0.00660111, 0.75432519, 0.25371165, 0.71735252]))]"
      ]
     },
     "execution_count": 1669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input_size = 2\n",
    "layer_output_sizes = [3, 4]\n",
    "activation_funcs = [sigmoid, ReLU]\n",
    "activation_ders = [sigmoid_der, ReLU_der]\n",
    "\n",
    "layers = create_layers(network_input_size, layer_output_sizes)\n",
    "\n",
    "x = np.random.rand(network_input_size)\n",
    "target = np.random.rand(4)\n",
    "\n",
    "layer_grads = backpropagation(x, layers, activation_funcs, target, activation_ders)\n",
    "print(layer_grads)\n",
    "\n",
    "cost_grad = grad(cost, 0)\n",
    "cost_grad(layers, x, [sigmoid, ReLU], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1670,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6\n",
    "def create_layers_batch(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "\n",
    "    i_size = network_input_size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        W = np.random.randn(layer_output_size, i_size).T\n",
    "        b = np.random.randn(layer_output_size)\n",
    "        layers.append((W, b))\n",
    "\n",
    "        i_size = layer_output_size\n",
    "    return layers\n",
    "\n",
    "def feed_forward_batch(inputs, layers, activation_funcs):\n",
    "    a = inputs\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = a @ W + b\n",
    "        a = activation_func(z)\n",
    "    return a\n",
    "\n",
    "def cost_batch(layers, inputs, activation_funcs, target):\n",
    "    predict = feed_forward_batch(inputs, layers, activation_funcs)\n",
    "    return mse(predict, target)\n",
    "\n",
    "def feed_forward_saver_batch(inputs, layers, activation_funcs):\n",
    "    layer_inputs = []\n",
    "    zs = []\n",
    "    a = inputs\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        layer_inputs.append(a)\n",
    "        z = a @ W + b\n",
    "        a = activation_func(z)\n",
    "\n",
    "        zs.append(z)\n",
    "\n",
    "    return layer_inputs, zs, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1671,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation_batch(inputs, layers, activation_funcs, target, activation_ders, cost_der=mse_der):\n",
    "    layer_inputs, zs, predict = feed_forward_saver_batch(inputs, layers, activation_funcs)\n",
    "\n",
    "    layer_grads = [None] * len(layers)\n",
    "\n",
    "    for i in reversed(range(len(layers))):\n",
    "        layer_input, z, activation_der = layer_inputs[i], zs[i], activation_ders[i]\n",
    "\n",
    "        if i == len(layers) - 1:\n",
    "            # For last layer we use cost derivative as dC_da(L) can be computed directly\n",
    "            dC_da = cost_der(predict, target)\n",
    "        else:\n",
    "            # For other layers we build on previous z derivative, as dC_da(i) = dC_dz(i+1) * dz(i+1)_da(i)\n",
    "            (W, b) = layers[i + 1]\n",
    "            dC_da = dC_dz @ W.T\n",
    "\n",
    "        dC_dz = dC_da * activation_der(z)\n",
    "        dC_dW = layer_input.T @ dC_dz / len(layers[-1][1])\n",
    "        dC_db = np.mean(dC_dz, axis=0) / len(layers[-1][1]) * len(layer_input) # deriv wrt b is 1\n",
    "\n",
    "        layer_grads[i] = (dC_dW, dC_db)\n",
    "\n",
    "    return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints: 17\n",
      "Network input size: 19\n",
      "Final output size: 7\n",
      "Our gradients:\n",
      "0 [ 1.68184154e-03 -2.09762290e-03  6.76325518e-03  4.85893527e-05\n",
      " -1.92748982e-03]\n",
      "1 [0.         0.04003649]\n",
      "2 [-0.00455653  0.00685776  0.01933652  0.00962605  0.01221039 -0.00951721\n",
      " -0.00247036]\n",
      "Autograd:\n",
      "0 [ 1.68184154e-03 -2.09762290e-03  6.76325518e-03  4.85893527e-05\n",
      " -1.92748982e-03]\n",
      "1 [0.         0.04003649]\n",
      "2 [-0.00455653  0.00685776  0.01933652  0.00962605  0.01221039 -0.00951721\n",
      " -0.00247036]\n"
     ]
    }
   ],
   "source": [
    "number_of_datapoints = np.random.randint(2, 20)\n",
    "network_input_size = np.random.randint(2, 20)\n",
    "final_output_size = np.random.randint(2, 20)\n",
    "\n",
    "inputs = np.random.rand(number_of_datapoints, network_input_size)\n",
    "layer_output_sizes = [5, 2, final_output_size]\n",
    "activation_funcs = [sigmoid, ReLU, sigmoid]\n",
    "activation_ders = [sigmoid_der, ReLU_der, sigmoid_der]\n",
    "\n",
    "layers = create_layers_batch(network_input_size, layer_output_sizes)\n",
    "\n",
    "target = np.random.rand(number_of_datapoints, final_output_size)\n",
    "\n",
    "layer_grads = backpropagation_batch(inputs, layers, activation_funcs, target, activation_ders)\n",
    "\n",
    "print(\"Number of datapoints:\", number_of_datapoints)\n",
    "print(\"Network input size:\", network_input_size)\n",
    "print(\"Final output size:\", final_output_size)\n",
    "\n",
    "print(\"Our gradients:\")\n",
    "for i in range(len(layer_grads)):\n",
    "    print(i, layer_grads[i][1])\n",
    "\n",
    "print(\"Autograd:\")\n",
    "cost_grad = grad(cost_batch, 0)\n",
    "w_autograd = cost_grad(layers, inputs, activation_funcs, target)\n",
    "for i in range(len(w_autograd)):\n",
    "    print(i, w_autograd[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7a)\n",
    "...\n",
    "\n",
    "# Exercise 7b)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were not able to complete exercise 7 in time, but will hopefully have time to update the submission during the weekend. If not, you will see the training implementation in the final project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
