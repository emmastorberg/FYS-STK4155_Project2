{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1502,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np  # We need to use this numpy wrapper to make automatic differentiation work later\n",
    "from autograd import grad, elementwise_grad\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Defining some activation functions\n",
    "def ReLU(z):\n",
    "    return np.where(z > 0, z, 0)\n",
    "\n",
    "# Derivative of the ReLU function\n",
    "def ReLU_der(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def mse(predict, target):\n",
    "    return np.mean((predict - target) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2a)\n",
    "\n",
    "The shape of weights and biases will be..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2b)\n",
    "def feed_forward_one_layer(W, b, x):\n",
    "    z = W @ x + b\n",
    "    a = sigmoid(z)\n",
    "    return a\n",
    "\n",
    "def cost_one_layer(W, b, x, target):\n",
    "    predict = feed_forward_one_layer(W, b, x)\n",
    "    return mse(predict, target)\n",
    "\n",
    "\n",
    "x = np.random.rand(2)\n",
    "target = np.random.rand(3)\n",
    "\n",
    "W = np.random.randn(len(target), len(x))\n",
    "b = np.random.randn(len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04104304 -0.02358647]\n",
      " [-0.0216656  -0.01245071]\n",
      " [-0.03184957 -0.01830319]] [-0.0686993  -0.03626466 -0.05331093]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2c)\n",
    "autograd_one_layer = grad(cost_one_layer, [0, 1])\n",
    "W_g, b_g = autograd_one_layer(W, b, x, target)\n",
    "print(W_g, b_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3a)\n",
    "\n",
    "The reusable results are dC/da and da/dz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38907269 -0.14674307 -0.41982237]\n",
      "[-0.38907269 -0.14674307 -0.41982237]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3b)\n",
    "z = W @ x + b\n",
    "a = sigmoid(z)\n",
    "\n",
    "predict = a\n",
    "\n",
    "def mse_der(predict, target):\n",
    "    return 2/len(predict) * (predict - target)\n",
    "\n",
    "print(mse_der(predict, target))\n",
    "\n",
    "cost_autograd = grad(mse, 0)\n",
    "print(cost_autograd(predict, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17657189 0.24713028 0.1269845 ]\n",
      "[0.17657189 0.24713028 0.1269845 ]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3c)\n",
    "def sigmoid_der(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "print(sigmoid_der(z))\n",
    "\n",
    "sigmoid_autograd = elementwise_grad(sigmoid, 0)\n",
    "print(sigmoid_autograd(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) (3,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3d) \n",
    "dC_da = mse_der(a, target)\n",
    "dC_dz = dC_da * sigmoid_der(z)\n",
    "\n",
    "print(dC_da.shape, dC_dz.shape)\n",
    "print(sigmoid_der(z).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3e)\n",
    "dz_dW = np.tensordot(np.eye(len(target)), x, axes=0)\n",
    "dz_db = np.ones(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04104304 -0.02358647]\n",
      " [-0.0216656  -0.01245071]\n",
      " [-0.03184957 -0.01830319]] [-0.0686993  -0.03626466 -0.05331093]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3f)\n",
    "dC_da = mse_der(a, target)\n",
    "dC_dz = dC_da * sigmoid_der(z)\n",
    "dC_dW = dC_dz @ dz_dW\n",
    "dC_db = dC_dz * dz_db\n",
    "\n",
    "print(dC_dW, dC_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04104304 -0.02358647]\n",
      " [-0.0216656  -0.01245071]\n",
      " [-0.03184957 -0.01830319]] [-0.0686993  -0.03626466 -0.05331093]\n"
     ]
    }
   ],
   "source": [
    "W_g, b_g = autograd_one_layer(W, b, x, target)\n",
    "print(W_g, b_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1511,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(2)\n",
    "target = np.random.rand(4)\n",
    "\n",
    "W1 = np.random.rand(3, 2)\n",
    "b1 = np.random.rand(3)\n",
    "\n",
    "W2 = np.random.rand(4, 3)\n",
    "b2 = np.random.rand(4)\n",
    "\n",
    "layers = [(W1, b1), (W2, b2)]\n",
    "\n",
    "z1 = W1 @ x + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = W2 @ a1 + b2\n",
    "a2 = sigmoid(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4a)\n",
    "dC_da2 = mse_der(a2, target) # OK\n",
    "dC_dz2 = dC_da2 * sigmoid_der(z2) # check vector as exponent\n",
    "dC_dW2 = dC_dz2 @ np.tensordot(np.eye(len(z2)), a1, axes=0)\n",
    "dC_db2 = dC_dz2 # deriv wrt b2 is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4b)\n",
    "\n",
    "The derivative of the second layer intermediate z2 wrt. the first layer activation a1 is a row vector where each entry is the sum of the corresponding row in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00138134 0.00107522]\n",
      " [0.00103358 0.00080452]\n",
      " [0.00111479 0.00086774]] [0.00190858 0.00142808 0.00154029]\n",
      "[[ 0.00244499  0.00263709  0.00222029]\n",
      " [-0.02034845 -0.02194721 -0.01847842]\n",
      " [ 0.01041953  0.01123818  0.00946197]\n",
      " [-0.00081125 -0.00087499 -0.0007367 ]] [ 0.00329011 -0.027382    0.0140211  -0.00109167]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4c)\n",
    "dC_da1 = dC_dz2 @ W2 # OK\n",
    "dC_dz1 = dC_da1 * sigmoid_der(z1) # check vector as exponent\n",
    "dC_dW1 = dC_dz1 @ np.tensordot(np.eye(len(z1)), x, axes=0) # OK\n",
    "dC_db1 = dC_dz1 # deriv wrt b1 is 1\n",
    "\n",
    "print(dC_dW1, dC_db1)\n",
    "print(dC_dW2, dC_db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[0.00138134, 0.00107522],\n",
       "         [0.00103358, 0.00080452],\n",
       "         [0.00111479, 0.00086774]]),\n",
       "  array([0.00190858, 0.00142808, 0.00154029])),\n",
       " (array([[ 0.00244499,  0.00263709,  0.00222029],\n",
       "         [-0.02034845, -0.02194721, -0.01847842],\n",
       "         [ 0.01041953,  0.01123818,  0.00946197],\n",
       "         [-0.00081125, -0.00087499, -0.0007367 ]]),\n",
       "  array([ 0.00329011, -0.027382  ,  0.0140211 , -0.00109167]))]"
      ]
     },
     "execution_count": 1514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 4d)\n",
    "def feed_forward_two_layers(layers, x):\n",
    "    W1, b1 = layers[0]\n",
    "    z1 = W1 @ x + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    W2, b2 = layers[1]\n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    return a2\n",
    "\n",
    "def cost_two_layers(layers, x, target):\n",
    "    predict = feed_forward_two_layers(layers, x)\n",
    "    return mse(predict, target)\n",
    "\n",
    "\n",
    "grad_two_layers = grad(cost_two_layers, 0)\n",
    "grad_two_layers(layers, x, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4e)\n",
    "The first derivative (the cost function) will be used one time on the outer layer. On the layer in question, we differentiate wrt W or b, but for intermediate layers we differentiate the activation functions and application of weight and bias over and over, until we reach the layer we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layers(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "\n",
    "    i_size = network_input_size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        W = np.random.randn(layer_output_size, i_size)\n",
    "        b = np.random.randn(layer_output_size)\n",
    "        layers.append((W, b))\n",
    "\n",
    "        i_size = layer_output_size\n",
    "    return layers\n",
    "\n",
    "\n",
    "def feed_forward(input, layers, activation_funcs):\n",
    "    a = input\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = W @ a + b\n",
    "        a = activation_func(z)\n",
    "    return a\n",
    "\n",
    "def cost(layers, input, activation_funcs, target):\n",
    "    predict = feed_forward(input, layers, activation_funcs)\n",
    "    return mse(predict, target)\n",
    "\n",
    "def feed_forward_saver(input, layers, activation_funcs):\n",
    "    layer_inputs = []\n",
    "    zs = []\n",
    "    a = input\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        layer_inputs.append(a)\n",
    "        z = W @ a + b\n",
    "        a = activation_func(z)\n",
    "\n",
    "        zs.append(z)\n",
    "\n",
    "    return layer_inputs, zs, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5a)\n",
    "def backpropagation(\n",
    "    input, layers, activation_funcs, target, activation_ders, cost_der=mse_der\n",
    "):\n",
    "    layer_inputs, zs, predict = feed_forward_saver(input, layers, activation_funcs)\n",
    "\n",
    "    layer_grads = [() for layer in layers]\n",
    "\n",
    "    # We loop over the layers, from the last to the first\n",
    "    for i in reversed(range(len(layers))):\n",
    "        layer_input, z, activation_der = layer_inputs[i], zs[i], activation_ders[i]\n",
    "\n",
    "        if i == len(layers) - 1:\n",
    "            # For last layer we use cost derivative as dC_da(L) can be computed directly\n",
    "            dC_da = cost_der(predict, target)\n",
    "        else:\n",
    "            # For other layers we build on previous z derivative, as dC_da(i) = dC_dz(i+1) * dz(i+1)_da(i)\n",
    "            (W, b) = layers[i + 1]\n",
    "            dC_da = dC_dz @ W\n",
    "\n",
    "        dC_dz = dC_da * activation_der(z)\n",
    "        dC_dW = dC_dz @ np.tensordot(np.eye(len(z)), layer_input, axes=0)\n",
    "        dC_db = dC_dz # deriv wrt b is 1\n",
    "\n",
    "        layer_grads[i] = (dC_dW, dC_db)\n",
    "\n",
    "    return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[ 0.05562646,  0.06186254],\n",
      "       [ 0.06775757,  0.07535362],\n",
      "       [-0.00078398, -0.00087187]]), array([ 0.11330833,  0.13801881, -0.00159693])), (array([[0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.26950016, 0.728228  , 0.45796449]]), array([-0.        , -0.        , -0.        ,  0.82743724]))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[ 0.05562646,  0.06186254],\n",
       "         [ 0.06775757,  0.07535362],\n",
       "         [-0.00078398, -0.00087187]]),\n",
       "  array([ 0.11330833,  0.13801881, -0.00159693])),\n",
       " (array([[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.26950016, 0.728228  , 0.45796449]]),\n",
       "  array([0.        , 0.        , 0.        , 0.82743724]))]"
      ]
     },
     "execution_count": 1517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input_size = 2\n",
    "layer_output_sizes = [3, 4]\n",
    "activation_funcs = [sigmoid, ReLU]\n",
    "activation_ders = [sigmoid_der, ReLU_der]\n",
    "\n",
    "layers = create_layers(network_input_size, layer_output_sizes)\n",
    "\n",
    "x = np.random.rand(network_input_size)\n",
    "target = np.random.rand(4)\n",
    "\n",
    "layer_grads = backpropagation(x, layers, activation_funcs, target, activation_ders)\n",
    "print(layer_grads)\n",
    "\n",
    "cost_grad = grad(cost, 0)\n",
    "cost_grad(layers, x, [sigmoid, ReLU], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1518,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6\n",
    "def create_layers_batch(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "\n",
    "    i_size = network_input_size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        W = np.random.randn(layer_output_size, i_size).T\n",
    "        b = np.random.randn(layer_output_size)\n",
    "        layers.append((W, b))\n",
    "\n",
    "        i_size = layer_output_size\n",
    "    return layers\n",
    "\n",
    "def feed_forward_batch(inputs, layers, activation_funcs):\n",
    "    a = inputs\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = a @ W + b\n",
    "        a = activation_func(z)\n",
    "    return a\n",
    "\n",
    "def cost_batch(layers, inputs, activation_funcs, target):\n",
    "    predict = feed_forward_batch(inputs, layers, activation_funcs)\n",
    "    return mse(predict, target)\n",
    "\n",
    "def feed_forward_saver_batch(inputs, layers, activation_funcs):\n",
    "    layer_inputs = []\n",
    "    zs = []\n",
    "    a = inputs\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        layer_inputs.append(a)\n",
    "        z = a @ W + b\n",
    "        a = activation_func(z)\n",
    "\n",
    "        zs.append(z)\n",
    "\n",
    "    return layer_inputs, zs, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation_batch(inputs, layers, activation_funcs, target, activation_ders, cost_der=mse_der):\n",
    "    layer_inputs, zs, predict = feed_forward_saver_batch(inputs, layers, activation_funcs)\n",
    "\n",
    "    layer_grads = [None] * len(layers)\n",
    "\n",
    "    for i in reversed(range(len(layers))):\n",
    "        layer_input, z, activation_der = layer_inputs[i], zs[i], activation_ders[i]\n",
    "\n",
    "        if i == len(layers) - 1:\n",
    "            # For last layer we use cost derivative as dC_da(L) can be computed directly\n",
    "            dC_da = cost_der(predict, target)\n",
    "        else:\n",
    "            # For other layers we build on previous z derivative, as dC_da(i) = dC_dz(i+1) * dz(i+1)_da(i)\n",
    "            (W, b) = layers[i + 1]\n",
    "            dC_da = dC_dz @ W.T\n",
    "\n",
    "        dC_dz = dC_da * activation_der(z)\n",
    "        dC_dW = layer_input.T @ dC_dz / len(layers[-1][1])\n",
    "        dC_db = np.mean(dC_dz, axis=0) / len(layers[-1][1])*len(layer_input) # deriv wrt b is 1\n",
    "\n",
    "        layer_grads[i] = (dC_dW, dC_db)\n",
    "\n",
    "    return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints: 15\n",
      "Network input size: 14\n",
      "Final output size: 4\n",
      "Our gradients:\n",
      "0 [0. 0. 0. 0. 0.]\n",
      "1 [0. 0.]\n",
      "2 [-0.02373454  0.02146993 -0.00179589  0.01736758]\n",
      "Autograd:\n",
      "0 [0. 0. 0. 0. 0.]\n",
      "1 [0. 0.]\n",
      "2 [-0.02373454  0.02146993 -0.00179589  0.01736758]\n"
     ]
    }
   ],
   "source": [
    "number_of_datapoints = np.random.randint(2, 20)\n",
    "network_input_size = np.random.randint(2, 20)\n",
    "final_output_size = np.random.randint(2, 20)\n",
    "\n",
    "inputs = np.random.rand(number_of_datapoints, network_input_size)\n",
    "layer_output_sizes = [5, 2, final_output_size]\n",
    "activation_funcs = [sigmoid, ReLU, sigmoid]\n",
    "activation_ders = [sigmoid_der, ReLU_der, sigmoid_der]\n",
    "\n",
    "layers = create_layers_batch(network_input_size, layer_output_sizes)\n",
    "\n",
    "target = np.random.rand(number_of_datapoints, final_output_size)\n",
    "\n",
    "layer_grads = backpropagation_batch(inputs, layers, activation_funcs, target, activation_ders)\n",
    "\n",
    "print(\"Number of datapoints:\", number_of_datapoints)\n",
    "print(\"Network input size:\", network_input_size)\n",
    "print(\"Final output size:\", final_output_size)\n",
    "\n",
    "print(\"Our gradients:\")\n",
    "for i in range(len(layer_grads)):\n",
    "    print(i, layer_grads[i][1])\n",
    "\n",
    "print(\"Autograd:\")\n",
    "cost_grad = grad(cost_batch, 0)\n",
    "w_autograd = cost_grad(layers, inputs, activation_funcs, target)\n",
    "for i in range(len(w_autograd)):\n",
    "    print(i, w_autograd[i][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fysstk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
