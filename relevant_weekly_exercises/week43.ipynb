{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np  # We need to use this numpy wrapper to make automatic differentiation work later\n",
    "from autograd import grad, elementwise_grad\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Defining some activation functions\n",
    "def ReLU(z):\n",
    "    return np.where(z > 0, z, 0)\n",
    "\n",
    "# Derivative of the ReLU function\n",
    "def ReLU_der(z):\n",
    "    return np.diag(np.where(z > 0, 1, 0))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def mse(predict, target):\n",
    "    return np.mean((predict - target) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2a)\n",
    "\n",
    "The shape of weights and biases will be..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2b)\n",
    "def feed_forward_one_layer(W, b, x):\n",
    "    z = W @ x + b\n",
    "    a = sigmoid(z)\n",
    "    return a\n",
    "\n",
    "def cost_one_layer(W, b, x, target):\n",
    "    predict = feed_forward_one_layer(W, b, x)\n",
    "    return mse(predict, target)\n",
    "\n",
    "\n",
    "x = np.random.rand(2)\n",
    "target = np.random.rand(3)\n",
    "\n",
    "W = np.random.randn(len(target), len(x))\n",
    "b = np.random.randn(len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00699902 -0.01090799]\n",
      " [-0.01907937 -0.02973526]\n",
      " [ 0.00934513  0.01456442]] [-0.0167164  -0.04556904  0.02231985]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2c)\n",
    "autograd_one_layer = grad(cost_one_layer, [0, 1])\n",
    "W_g, b_g = autograd_one_layer(W, b, x, target)\n",
    "print(W_g, b_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3a)\n",
    "\n",
    "The reusable results are dC/da and da/dz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22523209 -0.18437859  0.09324845]\n",
      "[-0.22523209 -0.18437859  0.09324845]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3b)\n",
    "z = W @ x + b\n",
    "a = sigmoid(z)\n",
    "\n",
    "predict = a\n",
    "\n",
    "def mse_der(predict, target):\n",
    "    return 2/len(predict) * (predict - target).T\n",
    "\n",
    "print(mse_der(predict, target))\n",
    "\n",
    "cost_autograd = grad(mse, 0)\n",
    "print(cost_autograd(predict, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07421857 0.         0.        ]\n",
      " [0.         0.24714928 0.        ]\n",
      " [0.         0.         0.23935892]]\n",
      "[0.07421857 0.24714928 0.23935892]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3c)\n",
    "def sigmoid_der(z):\n",
    "    return np.diag(np.exp(-z) / (1 + np.exp(-z))**2)\n",
    "\n",
    "print(sigmoid_der(z))\n",
    "\n",
    "sigmoid_autograd = elementwise_grad(sigmoid, 0)\n",
    "print(sigmoid_autograd(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) (3,)\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3d) \n",
    "dC_da = mse_der(a, target)\n",
    "dC_dz = dC_da @ sigmoid_der(z)\n",
    "\n",
    "print(dC_da.shape, dC_dz.shape)\n",
    "print(sigmoid_der(z).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3e)\n",
    "dz_dW = np.tensordot(np.eye(len(target)), x, axes=0)\n",
    "dz_db = np.ones(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00699902 -0.01090799]\n",
      " [-0.01907937 -0.02973526]\n",
      " [ 0.00934513  0.01456442]] [-0.0167164  -0.04556904  0.02231985]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3f)\n",
    "dC_da = mse_der(a, target)\n",
    "dC_dz = dC_da @ sigmoid_der(z)\n",
    "dC_dW = dC_dz @ dz_dW\n",
    "dC_db = dC_dz * dz_db\n",
    "\n",
    "print(dC_dW, dC_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00699902 -0.01090799]\n",
      " [-0.01907937 -0.02973526]\n",
      " [ 0.00934513  0.01456442]] [-0.0167164  -0.04556904  0.02231985]\n"
     ]
    }
   ],
   "source": [
    "W_g, b_g = autograd_one_layer(W, b, x, target)\n",
    "print(W_g, b_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(2)\n",
    "target = np.random.rand(4)\n",
    "\n",
    "W1 = np.random.rand(3, 2)\n",
    "b1 = np.random.rand(3)\n",
    "\n",
    "W2 = np.random.rand(4, 3)\n",
    "b2 = np.random.rand(4)\n",
    "\n",
    "layers = [(W1, b1), (W2, b2)]\n",
    "\n",
    "z1 = W1 @ x + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = W2 @ a1 + b2\n",
    "a2 = sigmoid(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4a)\n",
    "dC_da2 = mse_der(a2, target) # OK\n",
    "dC_dz2 = dC_da2 @ sigmoid_der(z2) # check vector as exponent\n",
    "dC_dW2 = dC_dz2 @ np.tensordot(np.eye(len(z2)), a1, axes=0)\n",
    "dC_db2 = dC_dz2 # deriv wrt b2 is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4b)\n",
    "\n",
    "The derivative of the second layer intermediate z2 wrt. the first layer activation a1 is a row vector where each entry is the sum of the corresponding row in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.18719449e-04 1.15675628e-04]\n",
      " [3.44727935e-04 7.68751211e-05]\n",
      " [4.28196777e-04 9.54888646e-05]] [0.0203002  0.013491   0.01675757]\n",
      "[[0.0112937  0.01193451 0.01380256]\n",
      " [0.02833034 0.0299378  0.03462383]\n",
      " [0.03609972 0.03814802 0.04411916]\n",
      " [0.01116046 0.01179371 0.01363972]] [0.02034986 0.05104778 0.06504725 0.02010978]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4c)\n",
    "dC_da1 = dC_dz2 @ W2 # OK\n",
    "dC_dz1 = dC_da1 @ sigmoid_der(z1) # check vector as exponent\n",
    "dC_dW1 = dC_dz1 @ np.tensordot(np.eye(len(z1)), x, axes=0) # OK\n",
    "dC_db1 = dC_dz1 # deriv wrt b1 is 1\n",
    "\n",
    "print(dC_dW1, dC_db1)\n",
    "print(dC_dW2, dC_db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[5.18719449e-04, 1.15675628e-04],\n",
       "         [3.44727935e-04, 7.68751211e-05],\n",
       "         [4.28196777e-04, 9.54888646e-05]]),\n",
       "  array([0.0203002 , 0.013491  , 0.01675757])),\n",
       " (array([[0.0112937 , 0.01193451, 0.01380256],\n",
       "         [0.02833034, 0.0299378 , 0.03462383],\n",
       "         [0.03609972, 0.03814802, 0.04411916],\n",
       "         [0.01116046, 0.01179371, 0.01363972]]),\n",
       "  array([0.02034986, 0.05104778, 0.06504725, 0.02010978]))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 4d)\n",
    "def feed_forward_two_layers(layers, x):\n",
    "    W1, b1 = layers[0]\n",
    "    z1 = W1 @ x + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    W2, b2 = layers[1]\n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    return a2\n",
    "\n",
    "def cost_two_layers(layers, x, target):\n",
    "    predict = feed_forward_two_layers(layers, x)\n",
    "    return mse(predict, target)\n",
    "\n",
    "\n",
    "grad_two_layers = grad(cost_two_layers, 0)\n",
    "grad_two_layers(layers, x, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4e)\n",
    "The first derivative (the cost function) will be used one time on the outer layer. On the layer in question, we differentiate wrt W or b, but for intermediate layers we differentiate the activation functions and application of weight and bias over and over, until we reach the layer we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layers(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "\n",
    "    i_size = network_input_size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        W = np.random.randn(layer_output_size, i_size)\n",
    "        b = np.random.randn(layer_output_size)\n",
    "        layers.append((W, b))\n",
    "\n",
    "        i_size = layer_output_size\n",
    "    return layers\n",
    "\n",
    "\n",
    "def feed_forward(input, layers, activation_funcs):\n",
    "    a = input\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = W @ a + b\n",
    "        a = activation_func(z)\n",
    "    return a\n",
    "\n",
    "def cost(layers, input, activation_funcs, target):\n",
    "    predict = feed_forward(input, layers, activation_funcs)\n",
    "    return mse(predict, target)\n",
    "\n",
    "def feed_forward_saver(input, layers, activation_funcs):\n",
    "    layer_inputs = []\n",
    "    zs = []\n",
    "    a = input\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        layer_inputs.append(a)\n",
    "        z = W @ a + b\n",
    "        a = activation_func(z)\n",
    "\n",
    "        zs.append(z)\n",
    "\n",
    "    return layer_inputs, zs, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5a)\n",
    "def backpropagation(\n",
    "    input, layers, activation_funcs, target, activation_ders, cost_der=mse_der\n",
    "):\n",
    "    layer_inputs, zs, predict = feed_forward_saver(input, layers, activation_funcs)\n",
    "\n",
    "    layer_grads = [() for layer in layers]\n",
    "\n",
    "    # We loop over the layers, from the last to the first\n",
    "    for i in reversed(range(len(layers))):\n",
    "        layer_input, z, activation_der = layer_inputs[i], zs[i], activation_ders[i]\n",
    "\n",
    "        if i == len(layers) - 1:\n",
    "            # For last layer we use cost derivative as dC_da(L) can be computed directly\n",
    "            dC_da = cost_der(predict, target)\n",
    "        else:\n",
    "            # For other layers we build on previous z derivative, as dC_da(i) = dC_dz(i+1) * dz(i+1)_da(i)\n",
    "            (W, b) = layers[i + 1]\n",
    "            dC_da = dC_dz @ W\n",
    "\n",
    "        dC_dz = dC_da @ activation_der(z)\n",
    "        dC_dW = dC_dz @ np.tensordot(np.eye(len(z)), layer_input, axes=0)\n",
    "        dC_db = dC_dz # deriv wrt b is 1\n",
    "\n",
    "        layer_grads[i] = (dC_dW, dC_db)\n",
    "\n",
    "    return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[-0.00365208, -0.05748956],\n",
      "       [ 0.0144103 ,  0.22684106],\n",
      "       [ 0.02711306,  0.42680257]]), array([-0.06113624,  0.24123002,  0.45387548])), (array([[0.09984452, 0.29811681, 0.30916917],\n",
      "       [0.16402653, 0.4897521 , 0.50790913],\n",
      "       [0.22711308, 0.67811658, 0.70325702],\n",
      "       [0.13157124, 0.39284675, 0.40741112]]), array([0.39310158, 0.64579494, 0.89417534, 0.518014  ]))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[-0.00365208, -0.05748956],\n",
       "         [ 0.0144103 ,  0.22684106],\n",
       "         [ 0.02711306,  0.42680257]]),\n",
       "  array([-0.06113624,  0.24123002,  0.45387548])),\n",
       " (array([[0.09984452, 0.29811681, 0.30916917],\n",
       "         [0.16402653, 0.4897521 , 0.50790913],\n",
       "         [0.22711308, 0.67811658, 0.70325702],\n",
       "         [0.13157124, 0.39284675, 0.40741112]]),\n",
       "  array([0.39310158, 0.64579494, 0.89417534, 0.518014  ]))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input_size = 2\n",
    "layer_output_sizes = [3, 4]\n",
    "activation_funcs = [sigmoid, ReLU]\n",
    "activation_ders = [sigmoid_der, ReLU_der]\n",
    "\n",
    "layers = create_layers(network_input_size, layer_output_sizes)\n",
    "\n",
    "x = np.random.rand(network_input_size)\n",
    "target = np.random.rand(4)\n",
    "\n",
    "layer_grads = backpropagation(x, layers, activation_funcs, target, activation_ders)\n",
    "print(layer_grads)\n",
    "\n",
    "cost_grad = grad(cost, 0)\n",
    "cost_grad(layers, x, [sigmoid, ReLU], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6\n",
    "def create_layers(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "\n",
    "    i_size = network_input_size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        W = np.random.randn(layer_output_size, i_size)\n",
    "        b = np.random.randn(layer_output_size)\n",
    "        layers.append((W, b))\n",
    "\n",
    "        i_size = layer_output_size\n",
    "    return layers\n",
    "\n",
    "def create_layers_batch(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "\n",
    "    i_size = network_input_size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        W = np.random.randn(layer_output_size, i_size).T\n",
    "        b = np.random.randn(layer_output_size)\n",
    "        layers.append((W, b))\n",
    "\n",
    "        i_size = layer_output_size\n",
    "    return layers\n",
    "\n",
    "def feed_forward(input, layers, activation_funcs):\n",
    "    a = input\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = W @ a + b\n",
    "        a = activation_func(z)\n",
    "    return a\n",
    "\n",
    "def feed_forward_batch(inputs, layers, activation_funcs):\n",
    "    a = inputs\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = a @ W + b\n",
    "        a = activation_func(z)\n",
    "    return a\n",
    "\n",
    "def cost(layers, input, activation_funcs, target):\n",
    "    predict = feed_forward(input, layers, activation_funcs)\n",
    "    return mse(predict, target)\n",
    "\n",
    "def cost_batch(layers, inputs, activation_funcs, target):\n",
    "    predict = feed_forward_batch(inputs, layers, activation_funcs)\n",
    "    return np.sum(-target * np.log(predict)) # NOT THE CORRECT COST FUNCTION\n",
    "\n",
    "def feed_forward_saver_batch(inputs, layers, activation_funcs):\n",
    "    layer_inputs = []\n",
    "    zs = []\n",
    "    a = inputs\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        layer_inputs.append(a)\n",
    "        z = a @ W + b\n",
    "        a = activation_func(z)\n",
    "\n",
    "        zs.append(z)\n",
    "\n",
    "    return layer_inputs, zs, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation_batch(\n",
    "    input, layers, activation_funcs, target, activation_ders, cost_der=mse_der\n",
    "):\n",
    "    layer_inputs, zs, predict = feed_forward_saver_batch(input, layers, activation_funcs)\n",
    "\n",
    "    layer_grads = [() for layer in layers]\n",
    "\n",
    "    # We loop over the layers, from the last to the first\n",
    "    for i in reversed(range(len(layers))):\n",
    "        layer_input, z, activation_der = layer_inputs[i], zs[i], activation_ders[i]\n",
    "\n",
    "        if i == len(layers) - 1:\n",
    "            # For last layer we use cost derivative as dC_da(L) can be computed directly\n",
    "            dC_da = cost_der(predict, target)\n",
    "        else:\n",
    "            # For other layers we build on previous z derivative, as dC_da(i) = dC_dz(i+1) * dz(i+1)_da(i)\n",
    "            (W, b) = layers[i + 1]\n",
    "            dC_da = dC_dz @ W\n",
    "\n",
    "        print(dC_da)\n",
    "        print(activation_der(z))\n",
    "        dC_dz = dC_da @ activation_der(z)\n",
    "        dC_dW = dC_dz @ np.tensordot(np.eye(len(z)), layer_input, axes=0)\n",
    "        dC_db = dC_dz # deriv wrt b is 1\n",
    "\n",
    "        layer_grads[i] = (dC_dW, dC_db)\n",
    "\n",
    "    return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'autograd.numpy' has no attribute 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m(\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m network_input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'autograd.numpy' has no attribute 'seed'"
     ]
    }
   ],
   "source": [
    "np.seed(8)\n",
    "\n",
    "inputs = np.random.rand(10, 2)\n",
    "network_input_size = 2\n",
    "layer_output_sizes = [3, 4]\n",
    "activation_funcs = [sigmoid, ReLU]\n",
    "activation_ders = [sigmoid_der, ReLU_der]\n",
    "\n",
    "layers = create_layers_batch(network_input_size, layer_output_sizes)\n",
    "\n",
    "x = np.random.rand(network_input_size)\n",
    "target = np.random.rand(4)\n",
    "\n",
    "layer_grads = backpropagation_batch(inputs, layers, activation_funcs, target, activation_ders)\n",
    "print(layer_grads)\n",
    "\n",
    "cost_grad = grad(cost, 0)\n",
    "cost_grad(layers, x, [sigmoid, ReLU], target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fysstk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
