{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np  # We need to use this numpy wrapper to make automatic differentiation work later\n",
    "from autograd import grad, elementwise_grad\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Defining some activation functions\n",
    "def ReLU(z):\n",
    "    return np.where(z > 0, z, 0)\n",
    "\n",
    "# Derivative of the ReLU function\n",
    "def ReLU_der(z):\n",
    "    return np.diag(np.where(z > 0, 1, 0))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def mse(predict, target):\n",
    "    return np.mean((predict - target) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2a)\n",
    "\n",
    "The shape of weights and biases will be..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2b)\n",
    "def feed_forward_one_layer(W, b, x):\n",
    "    z = W @ x + b\n",
    "    a = sigmoid(z)\n",
    "    return a\n",
    "\n",
    "def cost_one_layer(W, b, x, target):\n",
    "    predict = feed_forward_one_layer(W, b, x)\n",
    "    return mse(predict, target)\n",
    "\n",
    "\n",
    "x = np.random.rand(2)\n",
    "target = np.random.rand(3)\n",
    "\n",
    "W = np.random.randn(len(target), len(x))\n",
    "b = np.random.randn(len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00071956 0.04156912]\n",
      " [0.00119779 0.06919653]\n",
      " [0.0005225  0.03018487]] [0.04217315 0.07020202 0.03062348]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2c)\n",
    "autograd_one_layer = grad(cost_one_layer, [0, 1])\n",
    "W_g, b_g = autograd_one_layer(W, b, x, target)\n",
    "print(W_g, b_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3a)\n",
    "\n",
    "The reusable results are dC/da and da/dz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23130578 0.49200586 0.13473569]\n",
      "[0.23130578 0.49200586 0.13473569]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3b)\n",
    "z = W @ x + b\n",
    "a = sigmoid(z)\n",
    "\n",
    "predict = a\n",
    "\n",
    "def mse_der(predict, target):\n",
    "    return 2/len(predict) * (predict - target).T\n",
    "\n",
    "print(mse_der(predict, target))\n",
    "\n",
    "cost_autograd = grad(mse, 0)\n",
    "print(cost_autograd(predict, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1823264  0.         0.        ]\n",
      " [0.         0.14268532 0.        ]\n",
      " [0.         0.         0.22728557]]\n",
      "[0.1823264  0.14268532 0.22728557]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3c)\n",
    "def sigmoid_der(z):\n",
    "    return np.diag(np.exp(-z) / (1 + np.exp(-z))**2)\n",
    "\n",
    "print(sigmoid_der(z))\n",
    "\n",
    "sigmoid_autograd = elementwise_grad(sigmoid, 0)\n",
    "print(sigmoid_autograd(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) (3,)\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3d) \n",
    "dC_da = mse_der(a, target)\n",
    "dC_dz = dC_da @ sigmoid_der(z)\n",
    "\n",
    "print(dC_da.shape, dC_dz.shape)\n",
    "print(sigmoid_der(z).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3e)\n",
    "dz_dW = np.tensordot(np.eye(len(target)), x, axes=0)\n",
    "dz_db = np.ones(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00071956 0.04156912]\n",
      " [0.00119779 0.06919653]\n",
      " [0.0005225  0.03018487]] [0.04217315 0.07020202 0.03062348]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3f)\n",
    "dC_da = mse_der(a, target)\n",
    "dC_dz = dC_da @ sigmoid_der(z)\n",
    "dC_dW = dC_dz @ dz_dW\n",
    "dC_db = dC_dz * dz_db\n",
    "\n",
    "print(dC_dW, dC_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00071956 0.04156912]\n",
      " [0.00119779 0.06919653]\n",
      " [0.0005225  0.03018487]] [0.04217315 0.07020202 0.03062348]\n"
     ]
    }
   ],
   "source": [
    "W_g, b_g = autograd_one_layer(W, b, x, target)\n",
    "print(W_g, b_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(2)\n",
    "target = np.random.rand(4)\n",
    "\n",
    "W1 = np.random.rand(3, 2)\n",
    "b1 = np.random.rand(3)\n",
    "\n",
    "W2 = np.random.rand(4, 3)\n",
    "b2 = np.random.rand(4)\n",
    "\n",
    "layers = [(W1, b1), (W2, b2)]\n",
    "\n",
    "z1 = W1 @ x + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = W2 @ a1 + b2\n",
    "a2 = sigmoid(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4a)\n",
    "dC_da2 = mse_der(a2, target) # OK\n",
    "dC_dz2 = dC_da2 @ sigmoid_der(z2) # check vector as exponent\n",
    "dC_dW2 = dC_dz2 @ np.tensordot(np.eye(len(z2)), a1, axes=0)\n",
    "dC_db2 = dC_dz2 # deriv wrt b2 is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4b)\n",
    "\n",
    "The derivative of the second layer intermediate z2 wrt. the first layer activation a1 is a row vector where each entry is the sum of the corresponding row in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00186133 0.00228897]\n",
      " [0.00066551 0.00081841]\n",
      " [0.00173739 0.00213655]] [0.00319644 0.00114287 0.00298359]\n",
      "[[0.01307715 0.01134609 0.00998665]\n",
      " [0.00605096 0.00524998 0.00462095]\n",
      " [0.00256883 0.00222878 0.00196174]\n",
      " [0.00504424 0.00437652 0.00385215]] [0.0160403  0.00742205 0.00315089 0.00618721]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4c)\n",
    "dC_da1 = dC_dz2 @ W2 # OK\n",
    "dC_dz1 = dC_da1 @ sigmoid_der(z1) # check vector as exponent\n",
    "dC_dW1 = dC_dz1 @ np.tensordot(np.eye(len(z1)), x, axes=0) # OK\n",
    "dC_db1 = dC_dz1 # deriv wrt b1 is 1\n",
    "\n",
    "print(dC_dW1, dC_db1)\n",
    "print(dC_dW2, dC_db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[0.00186133, 0.00228897],\n",
       "         [0.00066551, 0.00081841],\n",
       "         [0.00173739, 0.00213655]]),\n",
       "  array([0.00319644, 0.00114287, 0.00298359])),\n",
       " (array([[0.01307715, 0.01134609, 0.00998665],\n",
       "         [0.00605096, 0.00524998, 0.00462095],\n",
       "         [0.00256883, 0.00222878, 0.00196174],\n",
       "         [0.00504424, 0.00437652, 0.00385215]]),\n",
       "  array([0.0160403 , 0.00742205, 0.00315089, 0.00618721]))]"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 4d)\n",
    "def feed_forward_two_layers(layers, x):\n",
    "    W1, b1 = layers[0]\n",
    "    z1 = W1 @ x + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    W2, b2 = layers[1]\n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    return a2\n",
    "\n",
    "def cost_two_layers(layers, x, target):\n",
    "    predict = feed_forward_two_layers(layers, x)\n",
    "    return mse(predict, target)\n",
    "\n",
    "\n",
    "grad_two_layers = grad(cost_two_layers, 0)\n",
    "grad_two_layers(layers, x, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4e)\n",
    "The first derivative (the cost function) will be used one time on the outer layer. On the layer in question, we differentiate wrt W or b, but for intermediate layers we differentiate the activation functions and application of weight and bias over and over, until we reach the layer we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layers(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "\n",
    "    i_size = network_input_size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        W = np.random.randn(layer_output_size, i_size)\n",
    "        b = np.random.randn(layer_output_size)\n",
    "        layers.append((W, b))\n",
    "\n",
    "        i_size = layer_output_size\n",
    "    return layers\n",
    "\n",
    "\n",
    "def feed_forward(input, layers, activation_funcs):\n",
    "    a = input\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = W @ a + b\n",
    "        a = activation_func(z)\n",
    "    return a\n",
    "\n",
    "def cost(layers, input, activation_funcs, target):\n",
    "    predict = feed_forward(input, layers, activation_funcs)\n",
    "    return mse(predict, target)\n",
    "\n",
    "def feed_forward_saver(input, layers, activation_funcs):\n",
    "    layer_inputs = []\n",
    "    zs = []\n",
    "    a = input\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        layer_inputs.append(a)\n",
    "        z = W @ a + b\n",
    "        a = activation_func(z)\n",
    "\n",
    "        zs.append(z)\n",
    "\n",
    "    return layer_inputs, zs, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5a)\n",
    "def backpropagation(\n",
    "    input, layers, activation_funcs, target, activation_ders, cost_der=mse_der\n",
    "):\n",
    "    layer_inputs, zs, predict = feed_forward_saver(input, layers, activation_funcs)\n",
    "\n",
    "    layer_grads = [() for layer in layers]\n",
    "\n",
    "    # We loop over the layers, from the last to the first\n",
    "    for i in reversed(range(len(layers))):\n",
    "        layer_input, z, activation_der = layer_inputs[i], zs[i], activation_ders[i]\n",
    "\n",
    "        if i == len(layers) - 1:\n",
    "            # For last layer we use cost derivative as dC_da(L) can be computed directly\n",
    "            dC_da = cost_der(predict, target)\n",
    "        else:\n",
    "            # For other layers we build on previous z derivative, as dC_da(i) = dC_dz(i+1) * dz(i+1)_da(i)\n",
    "            (W, b) = layers[i + 1]\n",
    "            dC_da = dC_dz @ W\n",
    "\n",
    "        dC_dz = dC_da @ activation_der(z)\n",
    "        dC_dW = dC_dz @ np.tensordot(np.eye(len(z)), layer_input, axes=0)\n",
    "        dC_db = dC_dz # deriv wrt b is 1\n",
    "\n",
    "        layer_grads[i] = (dC_dW, dC_db)\n",
    "\n",
    "    return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[ 0.09724526,  0.15364005],\n",
      "       [ 0.0366406 ,  0.05788934],\n",
      "       [-0.15925263, -0.25160695]]), array([ 0.16196256,  0.06102514, -0.26523622])), (array([[0.        , 0.        , 0.        ],\n",
      "       [0.06314564, 0.05086566, 0.05101922],\n",
      "       [0.41967321, 0.33805906, 0.33907962],\n",
      "       [0.02285746, 0.01841236, 0.01846794]]), array([0.        , 0.084113  , 0.55902472, 0.03044723]))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[ 0.09724526,  0.15364005],\n",
       "         [ 0.0366406 ,  0.05788934],\n",
       "         [-0.15925263, -0.25160695]]),\n",
       "  array([ 0.16196256,  0.06102514, -0.26523622])),\n",
       " (array([[0.        , 0.        , 0.        ],\n",
       "         [0.06314564, 0.05086566, 0.05101922],\n",
       "         [0.41967321, 0.33805906, 0.33907962],\n",
       "         [0.02285746, 0.01841236, 0.01846794]]),\n",
       "  array([0.        , 0.084113  , 0.55902472, 0.03044723]))]"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input_size = 2\n",
    "layer_output_sizes = [3, 4]\n",
    "activation_funcs = [sigmoid, ReLU]\n",
    "activation_ders = [sigmoid_der, ReLU_der]\n",
    "\n",
    "layers = create_layers(network_input_size, layer_output_sizes)\n",
    "\n",
    "x = np.random.rand(network_input_size)\n",
    "target = np.random.rand(4)\n",
    "\n",
    "layer_grads = backpropagation(x, layers, activation_funcs, target, activation_ders)\n",
    "print(layer_grads)\n",
    "\n",
    "cost_grad = grad(cost, 0)\n",
    "cost_grad(layers, x, [sigmoid, ReLU], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6\n",
    "def create_layers(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "\n",
    "    i_size = network_input_size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        W = np.random.randn(layer_output_size, i_size)\n",
    "        b = np.random.randn(layer_output_size)\n",
    "        layers.append((W, b))\n",
    "\n",
    "        i_size = layer_output_size\n",
    "    return layers\n",
    "\n",
    "def create_layers_batch(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "\n",
    "    i_size = network_input_size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        W = np.random.randn(layer_output_size, i_size).T\n",
    "        b = np.random.randn(layer_output_size)\n",
    "        layers.append((W, b))\n",
    "\n",
    "        i_size = layer_output_size\n",
    "    return layers\n",
    "\n",
    "def feed_forward(input, layers, activation_funcs):\n",
    "    a = input\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = W @ a + b\n",
    "        a = activation_func(z)\n",
    "    return a\n",
    "\n",
    "def feed_forward_batch(inputs, layers, activation_funcs):\n",
    "    a = inputs\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = a @ W + b\n",
    "        a = activation_func(z)\n",
    "    return a\n",
    "\n",
    "def cost(layers, input, activation_funcs, target):\n",
    "    predict = feed_forward(input, layers, activation_funcs)\n",
    "    return mse(predict, target)\n",
    "\n",
    "def cost_batch(layers, inputs, activation_funcs, target):\n",
    "    predict = feed_forward_batch(inputs, layers, activation_funcs)\n",
    "    return np.sum(-target * np.log(predict)) # NOT THE CORRECT COST FUNCTION\n",
    "\n",
    "def feed_forward_saver_batch(inputs, layers, activation_funcs):\n",
    "    layer_inputs = []\n",
    "    zs = []\n",
    "    a = inputs\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        layer_inputs.append(a)\n",
    "        z = a @ W + b\n",
    "        a = activation_func(z)\n",
    "\n",
    "        zs.append(z)\n",
    "\n",
    "    return layer_inputs, zs, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation_batch(\n",
    "    input, layers, activation_funcs, target, activation_ders, cost_der=mse_der\n",
    "):\n",
    "    layer_inputs, zs, predict = feed_forward_saver_batch(input, layers, activation_funcs)\n",
    "\n",
    "    layer_grads = [() for layer in layers]\n",
    "\n",
    "    # We loop over the layers, from the last to the first\n",
    "    for i in reversed(range(len(layers))):\n",
    "        layer_input, z, activation_der = layer_inputs[i], zs[i], activation_ders[i]\n",
    "\n",
    "        if i == len(layers) - 1:\n",
    "            # For last layer we use cost derivative as dC_da(L) can be computed directly\n",
    "            dC_da = cost_der(predict, target)\n",
    "        else:\n",
    "            # For other layers we build on previous z derivative, as dC_da(i) = dC_dz(i+1) * dz(i+1)_da(i)\n",
    "            (W, b) = layers[i + 1]\n",
    "            dC_da = dC_dz @ W\n",
    "\n",
    "        print(dC_da)\n",
    "        print(activation_der(z))\n",
    "        dC_dz = dC_da @ activation_der(z)\n",
    "        dC_dW = dC_dz @ np.tensordot(np.eye(len(z)), layer_input, axes=0)\n",
    "        dC_db = dC_dz # deriv wrt b is 1\n",
    "\n",
    "        layer_grads[i] = (dC_dW, dC_db)\n",
    "\n",
    "    return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02547315 -0.02836122 -0.02359569 -0.02044567 -0.02749925 -0.01731853\n",
      "  -0.03517198 -0.03761136 -0.03630514 -0.03540921]\n",
      " [-0.04641806 -0.04641806 -0.04641806 -0.04641806 -0.04641806 -0.04641806\n",
      "  -0.04641806 -0.04641806 -0.04641806 -0.04641806]\n",
      " [-0.10000785 -0.10000785 -0.10000785 -0.10000785 -0.10000785 -0.10000785\n",
      "  -0.10000785 -0.10000785 -0.10000785 -0.10000785]\n",
      " [ 0.15618219  0.18558829  0.17317283  0.18189949  0.05345909  0.14726545\n",
      "   0.04293775  0.0702213   0.17608983  0.15369665]]\n",
      "[1 0 0 1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[511], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(network_input_size)\n\u001b[1;32m     10\u001b[0m target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m layer_grads \u001b[38;5;241m=\u001b[39m \u001b[43mbackpropagation_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_funcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_ders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(layer_grads)\n\u001b[1;32m     15\u001b[0m cost_grad \u001b[38;5;241m=\u001b[39m grad(cost, \u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[510], line 18\u001b[0m, in \u001b[0;36mbackpropagation_batch\u001b[0;34m(input, layers, activation_funcs, target, activation_ders, cost_der)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# For other layers we build on previous z derivative, as dC_da(i) = dC_dz(i+1) * dz(i+1)_da(i)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     (W, b) \u001b[38;5;241m=\u001b[39m layers[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m     dC_da \u001b[38;5;241m=\u001b[39m \u001b[43mdC_dz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(dC_da)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(activation_der(z))\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 10)"
     ]
    }
   ],
   "source": [
    "inputs = np.random.rand(10, 2)\n",
    "network_input_size = 2\n",
    "layer_output_sizes = [3, 4]\n",
    "activation_funcs = [sigmoid, ReLU]\n",
    "activation_ders = [sigmoid_der, ReLU_der]\n",
    "\n",
    "layers = create_layers_batch(network_input_size, layer_output_sizes)\n",
    "\n",
    "x = np.random.rand(network_input_size)\n",
    "target = np.random.rand(4)\n",
    "\n",
    "layer_grads = backpropagation_batch(inputs, layers, activation_funcs, target, activation_ders)\n",
    "print(layer_grads)\n",
    "\n",
    "cost_grad = grad(cost, 0)\n",
    "cost_grad(layers, x, [sigmoid, ReLU], target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
