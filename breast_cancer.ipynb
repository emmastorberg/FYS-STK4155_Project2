{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "from GradientDescent import Stochastic\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_layers(network_input_size, layer_output_sizes):\n",
    "#     layers = []\n",
    "\n",
    "#     i_size = network_input_size\n",
    "#     for layer_output_size in layer_output_sizes:\n",
    "#         W = np.random.randn(layer_output_size, i_size).T * 0.001\n",
    "#         b = np.random.randn(layer_output_size)\n",
    "#         layers.append((W, b))\n",
    "\n",
    "#         i_size = layer_output_size\n",
    "#     return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feed_forward(inputs, layers, activation_funcs):\n",
    "#     a = inputs\n",
    "#     for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "#         z = a @ W + b\n",
    "#         a = activation_func(z)\n",
    "#     return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(z):\n",
    "#     neg = np.where(z < 0, np.exp(z) / (1 + np.exp(z)), 0)\n",
    "#     pos = np.where(z >= 0, 1 / (1 + np.exp(-z)), 0)\n",
    "#     return pos + neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_entropy(predict, target):\n",
    "#     predict = np.clip(predict, 1e-8, 1 - 1e-8)\n",
    "#     return np.sum(-target * np.log(predict))\n",
    "\n",
    "# def cost(input, layers, activation_funcs, target):\n",
    "#     prediction = feed_forward(input, layers, activation_funcs)\n",
    "#     return cross_entropy(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network_input_size = 30\n",
    "# layer_output_sizes = [1]\n",
    "# activation_funcs = [sigmoid]\n",
    "# backprop = grad(cost, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancer = load_breast_cancer()\n",
    "# inputs = cancer.data\n",
    "# targets = cancer.target\n",
    "\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(inputs, targets)\n",
    "# x_train, x_test, y_train, y_test = x_train[:,:], x_test[:,:], y_train[:], y_test[:]\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# x_train = scaler.fit_transform(x_train, y_train)\n",
    "# x_test = scaler.transform(x_test)\n",
    "\n",
    "# layers = create_layers(network_input_size, layer_output_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0.001\n",
    "# for i in range(10000):\n",
    "#     gradient = backprop(x_train, layers, activation_funcs, y_train)\n",
    "#     dW, db = gradient[0]\n",
    "#     W, b = layers[0]\n",
    "#     W -= lr * dW\n",
    "#     b -= lr * db\n",
    "#     layers = [(W, b)]\n",
    "\n",
    "#     if int(i % 10) == 0:\n",
    "#         prediction = feed_forward(x_train, layers, activation_funcs)\n",
    "#         print(cross_entropy(prediction, y_train))\n",
    "\n",
    "# prediction = feed_forward(x_train, layers, activation_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cold = np.where(prediction >= 0.5, 1, 0) + np.where(prediction < 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for elem, targ in zip(cold, y_train):\n",
    "#     print(f\"pred: {elem[0]}       targ: {targ}\")\n",
    "\n",
    "# for pred, targ in zip(prediction, y_train):\n",
    "#     print(f\"pred: {pred[0]}       targ: {targ}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 30 is different from 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m backprop \u001b[38;5;241m=\u001b[39m grad(cost, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m---> 49\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m \u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Update weights and biases for all layers\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(layers)):\n",
      "File \u001b[0;32m~/Documents/MAMI/FYS-STK3155/projects/.fysstk/lib/python3.12/site-packages/autograd/wrap_util.py:20\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m argnum)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munary_operator\u001b[49m\u001b[43m(\u001b[49m\u001b[43munary_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnary_op_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnary_op_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MAMI/FYS-STK3155/projects/.fysstk/lib/python3.12/site-packages/autograd/differential_operators.py:28\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;129m@unary_to_nary\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad\u001b[39m(fun, x):\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    Returns a function which computes the gradient of `fun` with respect to\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    positional argument number `argnum`. The returned function takes the same\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    arguments as `fun`, but returns the gradient instead. The function `fun`\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    should be scalar-valued. The gradient has the same type as the argument.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     vjp, ans \u001b[38;5;241m=\u001b[39m \u001b[43m_make_vjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vspace(ans)\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrad only applies to real scalar-output functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry jacobian, elementwise_grad or holomorphic_grad.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/MAMI/FYS-STK3155/projects/.fysstk/lib/python3.12/site-packages/autograd/core.py:10\u001b[0m, in \u001b[0;36mmake_vjp\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_vjp\u001b[39m(fun, x):\n\u001b[1;32m      9\u001b[0m     start_node \u001b[38;5;241m=\u001b[39m VJPNode\u001b[38;5;241m.\u001b[39mnew_root()\n\u001b[0;32m---> 10\u001b[0m     end_value, end_node \u001b[38;5;241m=\u001b[39m  \u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(g): \u001b[38;5;28;01mreturn\u001b[39;00m vspace(x)\u001b[38;5;241m.\u001b[39mzeros()\n",
      "File \u001b[0;32m~/Documents/MAMI/FYS-STK3155/projects/.fysstk/lib/python3.12/site-packages/autograd/tracer.py:10\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(start_node, fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace_stack\u001b[38;5;241m.\u001b[39mnew_trace() \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[1;32m      9\u001b[0m     start_box \u001b[38;5;241m=\u001b[39m new_box(x, t, start_node)\n\u001b[0;32m---> 10\u001b[0m     end_box \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_box\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isbox(end_box) \u001b[38;5;129;01mand\u001b[39;00m end_box\u001b[38;5;241m.\u001b[39m_trace \u001b[38;5;241m==\u001b[39m start_box\u001b[38;5;241m.\u001b[39m_trace:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m end_box\u001b[38;5;241m.\u001b[39m_value, end_box\u001b[38;5;241m.\u001b[39m_node\n",
      "File \u001b[0;32m~/Documents/MAMI/FYS-STK3155/projects/.fysstk/lib/python3.12/site-packages/autograd/wrap_util.py:15\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f.<locals>.unary_f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     subargs \u001b[38;5;241m=\u001b[39m subvals(args, \u001b[38;5;28mzip\u001b[39m(argnum, x))\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msubargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 32\u001b[0m, in \u001b[0;36mcost\u001b[0;34m(input, layers, target)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcost\u001b[39m(\u001b[38;5;28minput\u001b[39m, layers, target):\n\u001b[0;32m---> 32\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_funcs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy(prediction, target)\n",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m, in \u001b[0;36mfeed_forward\u001b[0;34m(inputs, layers, activation_funcs)\u001b[0m\n\u001b[1;32m     12\u001b[0m a \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (W, b), activation_func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(layers, activation_funcs):\n\u001b[0;32m---> 14\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m     15\u001b[0m     a \u001b[38;5;241m=\u001b[39m activation_func(z)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[0;32m~/Documents/MAMI/FYS-STK3155/projects/.fysstk/lib/python3.12/site-packages/autograd/numpy/numpy_boxes.py:33\u001b[0m, in \u001b[0;36mArrayBox.__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__matmul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43manp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MAMI/FYS-STK3155/projects/.fysstk/lib/python3.12/site-packages/autograd/tracer.py:44\u001b[0m, in \u001b[0;36mprimitive.<locals>.f_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m parents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(box\u001b[38;5;241m.\u001b[39m_node \u001b[38;5;28;01mfor\u001b[39;00m _     , box \u001b[38;5;129;01min\u001b[39;00m boxed_args)\n\u001b[1;32m     43\u001b[0m argnums \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(argnum    \u001b[38;5;28;01mfor\u001b[39;00m argnum, _   \u001b[38;5;129;01min\u001b[39;00m boxed_args)\n\u001b[0;32m---> 44\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mf_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m node \u001b[38;5;241m=\u001b[39m node_constructor(ans, f_wrapped, argvals, kwargs, argnums, parents)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_box(ans, trace, node)\n",
      "File \u001b[0;32m~/Documents/MAMI/FYS-STK3155/projects/.fysstk/lib/python3.12/site-packages/autograd/tracer.py:48\u001b[0m, in \u001b[0;36mprimitive.<locals>.f_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_box(ans, trace, node)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 30 is different from 100)"
     ]
    }
   ],
   "source": [
    "def create_layers(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "    i_size = network_input_size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        std = np.sqrt(2 / (layer_output_size + i_size))\n",
    "        W = np.random.normal(scale=std, size=(i_size, layer_output_size))\n",
    "        b = np.zeros(layer_output_size)\n",
    "        layers.append((W, b))\n",
    "    return layers\n",
    "\n",
    "def feed_forward(inputs, layers, activation_funcs):\n",
    "    a = inputs\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = a @ W + b\n",
    "        a = activation_func(z)\n",
    "    return a\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def cross_entropy(predict, target):\n",
    "    predict = np.clip(predict, 1e-10, 1 - 1e-10)\n",
    "    target = target\n",
    "    return -np.sum(target * np.log(predict) + (1 - target) * np.log(1 - predict))\n",
    "\n",
    "network_input_size = 30\n",
    "n_layers = 5\n",
    "layer_output_sizes = [100] * (n_layers-1) + [1]\n",
    "activation_funcs = [sigmoid] * n_layers\n",
    "\n",
    "def cost(input, layers, target):\n",
    "    prediction = feed_forward(input, layers, activation_funcs)\n",
    "    return cross_entropy(prediction, target)\n",
    "\n",
    "inputs, targets = utils.get_cancer_data()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, targets)\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "layers = create_layers(network_input_size, layer_output_sizes)\n",
    "\n",
    "lr = 0.0001\n",
    "\n",
    "backprop = grad(cost, 1)\n",
    "\n",
    "for i in range(10000):\n",
    "    gradient = backprop(x_train, layers, y_train)\n",
    "\n",
    "    # Update weights and biases for all layers\n",
    "    for j in range(len(layers)):\n",
    "        dW, db = gradient[j]\n",
    "        \n",
    "        # Clip gradients\n",
    "        dW = np.clip(dW, -1, 1)\n",
    "        db = np.clip(db, -1, 1)\n",
    "        \n",
    "        W, b = layers[j]\n",
    "        W -= lr * dW\n",
    "        b -= lr * db\n",
    "        layers[j] = (W, b)\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        prediction = feed_forward(x_train, layers, activation_funcs)\n",
    "        print(f\"Iteration {i}, Cost: {cross_entropy(prediction, y_train)}\")\n",
    "\n",
    "\n",
    "prediction = feed_forward(x_train, layers, activation_funcs)\n",
    "\n",
    "for pred, targ in zip(prediction, y_train):\n",
    "    print(f\"prediction: {pred}       target: {targ}\")\n",
    "\n",
    "print(f\"Final Cost: {cross_entropy(prediction, y_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fysstk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
