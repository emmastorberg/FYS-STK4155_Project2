This section documents the results of various experiments we did. Since the final aim of the
project is to predict breast cancer data accurately, we systematically tested all necessary components to perform this task, such that we ensure correctness (and reproducibility) of our final results. All plots show the performance of the model on testing data only.

\subsection{Linear Regression with Gradient Descent Methods}
In the first set of experiments, we are using a dataset in the form of a polynomial $f(x) = 5x^2 + 7x +3$ over the interval $x \in [0,1)$. We began with the most simple method, plain gradient descent, which we used with a constant learning rate. We tested the values $\eta \in \{ 0.1, 0.01, 0.001, 0.0001\}$ to find the best possible value to fixate for our experiments\footnote{The plots showing our testing can be found on our GitHub in the directory \texttt{figures/all\_plots}.}, and based on this test, we saw that we had the best results when $\eta = 0.1$. With this learning rate, we studied convergence speed towards the minimum when using plain gradient descent with and with momentum. The plot of this (and the comparison to the analytical solution) is in figure \ref{fig:plainVSanalytical}, up to 10 epochs \cite{mediumEpochNumber}\cite{epochsBreastCancerArticle}.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/plain_mse_pr_iter_eta_1e-1.png}
    \caption{Plain gradient descent (PGD) with and without momentum compared to the analytical solution, with a constant learning rate of $\eta = 0.1$ and a momentum parameter $\gamma = 0.5$.}
    \label{fig:plainVSanalytical}
\end{figure}

Next, we looked at our stochastic methods, with and without momentum. In this case we tried a linearly decaying learning rate to see if we would have any major improvements over plain gradient descent. In this case, the linearly decaying learning rate used was:
\[
\eta_t = (1- \frac{t}{5})0.1 + \frac{t}{5} 0.001
\]
where we found the value $\eta_0 = 0.1$ after a few tests, as well as consulting the literature. We also kept the mini-batch size fixed at 4, which we found produced decent values in this case. The comparison of the MSE with and without momentum as a function of epochs is displayed in figure \ref{fig:sgdVSanalytical}. We also found our that when timed, stochastic methods gave us results quicker than the plain versions, with minimal detriment to the MSE. Therefore, from this point on, we will continue using our stochastic methods for computational efficiency.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/sgd_mse_pr_epoch_eta_1e-1.png}
    \caption{Stochastic gradient descent (SGD) with and without momentum compared to the analytical solution, with a constant learning rate of $\eta_0 = 0.1$ and a momentum parameter $\gamma = 0.5$. The size of the mini-batches is also fixed at 4.}
    \label{fig:sgdVSanalytical}
\end{figure}

Next, we looked at the methods with adaptive learning rates. All methods were initialized with the same global learning rate $\eta = 0.1$ to begin with, as this was the best result in both of our previous tests. We wanted to see how the different methods of adaptive learning rate converged, and if this behavior was consistent with the theory. The development of the MSE over epochs is shown in figure \ref{fig:MSEGD}. We see faster convergence in methods with momentum.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/placeholders/learningratesGD.png}
%     \caption{\textcolor{purple}{Plot of 3 subplots showing SGD w and w/o momentum for each of the three methods of adaptable learning rates.  Function of number of epochs. Learning rate on y-axis. See photo on Emma's phone.}}
%     \label{fig:learningratesGD}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/MSEGD.png}
    \caption{MSE as a function of epochs for the different emthods of adaptive learnign rate. For all methods, mini-batch size is fixed at 4, and momentum terms where applicable are scaled by $\gamma = 0.5$. SGD decays linearly, as in the previous figure.}
    \label{fig:MSEGD}
\end{figure}

As part of our testing process, we compared the performance of our gradients to that of the automatic differentiation library Autograd \cite{autograd}. Since our tests pass\footnote{All code (including tests) can be found on our GitHub: \url{https://github.com/emmastorberg/FYS-STK4155_Project2/}}, we expected our implementations to give quite similar results as these pre-existing methods, which they did.
\vspace{3cm}

Up to this point, we have only used OLS. When we use ridge regression instead, there is the additional hyperparameter $\lambda$ to consider. We performed a grid search to determine what combination of $\lambda$ and gradient descent method to choose, and tested $\lambda \in \{10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}\}$ against five of the seven methods we tested for OLS in figure \ref{fig:MSEGD} (AdaGrad is excluded because of overall poor performance, as we also see in the figure \ref{fig:MSEGD}). The grid search table can be seen in figure \ref{fig:gridsearch_ridge}, where it is clear that smaller values of $\lambda$ do best, so we will use $\lambda = 0.00001$.

It is difficult to visually determine exactly which method is best, and upon reading off the values numerically, we also found minimal differences in the darkest regions. This is also consistent with the plots, where we see all methods except AdaGrad seem to converge nicely. Therefore, we determine that we will use Adam as we continue to use gradient descent in a neural network, as this seems in line with the literature on the subject.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/gridsearch_ridge.png}
    \caption{Grid search table of the seven possible methods compared for different values of the hyperparameter $\lambda$. In all cases, the global learning rate is set to 0.1, the mini-batch size is fixed at 4, and momentum terms are scaled by $\gamma = 0.5$.}
    \label{fig:gridsearch_ridge}
\end{figure}

\subsection{Numerical Prediction with Neural Network}
We saw that the optimal gradient descent methods for linear regression were RMSProp (with and without momentum) and Adam for OLS, and we chose Adam with $\lambda = 0.00001$ as the best-performing combination for ridge regression. 

There is an enormous number of possible neural networks we can create even with only the relatively few gradient descent methods and activation functions at our disposal, in addition to the limitation of computation time. We eliminated a few degrees of freedom here by sticking to Adam as our gradient descent method (based on its performance in the experiments up to this point), and with ReLU as the activation function for the outer layer, guided by the theoretical intuition of what values it can produce (only positive values; the range of our polynomial $f(x) = 5x^2 + 7x + 3$ is positive real numbers only).

To construct the network, we tried to sequentially eliminate degrees of freedom by fixating certain parameters, and thereafter exploring slight modifications to these. We started by using the same number of nodes per hidden layer, and performing a grid search to find the best combination. The results of this are shown in figure \ref{fig:gridsearch_numpred_layers_nodes}, with ReLU activating all hidden layers as well as the output layer. As we can see, there are many options that have low MSEs (dark color). We will choose a network with four nodes and four hidden layers for simplicity, which had an MSE of 0.09854. Also of note is that the lightest sections with high MSE are from networks that predict zero everywhere, which is an unexpected finding.  
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/gridsearch_numpred_layers_nodes.png}
    \caption{Grid search table showing the MSE of neural networks with varying number of hidden layers ($y$-axis), with a fixed number of nodes per layer ($x$-axis) and ReLU on all layers.}
    \label{fig:gridsearch_numpred_layers_nodes}
\end{figure}

Next, we tried varying what activation functions to use on the hidden layers. The plot of the resulting MSEs is shown in figure \ref{fig:activationfunctions_cost_numpred}. It looks like ReLU is (slightly) better for activating the hidden layers in this case, so we continued working with the same neural network with four hidden layers, four nodes per layer and ReLU activating everything. This became the basis for further modifications in hopes of improving performance. 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/activationfunctions_cost_numpred.png}
    \caption{MSE of a neural network of four hidden layers and four nodes per layer activated by different functions in the hidden layers (ReLU was always used on the output layer).}
    \label{fig:activationfunctions_cost_numpred}
\end{figure}

We attempted some slight variations to the hidden layers, the nodes in them and the activation functions at each stage, without significant improvement, so we stuck to the network we determined previously. Its specifications and hyperparameters can be seen in table \ref{tab:numericalprediction} below: 
\begin{table}[h!]
  \centering
  \small
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Hyperparameter} & \textbf{Value} \\
    \hline
    Number of data points & $n = 100$ \\
    \hline
    Scaling of data & \texttt{StandardScaler} \\
    \hline
    Mini-batch number (epochs) & $M = 4$ \\
    \hline
    Mini-batch size & $20$ (test data) \\
    \hline
    Optimizer & Adam \\
    \hline
    Initial learning rate & $\eta = 0.1$ \\
    \hline
    Decay rate of first moment & $\rho_1 = 0.9$ \\
    \hline
    Decay rate of second moment & $\rho_2 = 0.99$ \\
    \hline
    Numerical stability constant & $\delta = 10^{-8}$ \\
    \hline
    Cost function & MSE \\
    \hline
    Input layer & 1 node, ReLU \\
    \hline
    Hidden layer 1 & 4 nodes, ReLU \\
    \hline
    Hidden layer 2 & 4 nodes, ReLU \\
    \hline
    Hidden layer 3 & 4 nodes, ReLU \\
    \hline
    Hidden layer 4 & 4 nodes, ReLU \\
    \hline
    Output layer & 1 node, ReLU \\
    \hline
  \end{tabular}
  \caption{The chosen hyperparameters and other relevant values for best performance in numerical prediction task.}
  \label{tab:numericalprediction}
\end{table}

The neural network described above was able to predict with an MSE around 2.5 on the testing data, as we see in figure \ref{fig:numericalprediction}. This is overall a decent performance, but we also see that our linear regression methods (as well as ridge regression with Scikit-Learn) perform much better, with less tuning required. We also show the predictions made by each method in figure \ref{fig:predictioncomparison}. As we can see, most of the linear regression methods excel and are barely distinguishable from the exact solution, while the neural networks sticks out from the rest. 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/numericalprediction.png}
    \caption{MSE as a function of epochs (shown from 5 to 10 epochs) for our best versions of OLS, Ridge and neural network, all with the optimizer Adam. They are also compared to Scikit-Learn's ridge regression by SGD with $\eta = 0.1$.}
    \label{fig:numericalprediction}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/predictioncomparison.png}
    \caption{Comparison of predictions with optimal versions of all methods shown on scaled input data.}
    \label{fig:predictioncomparison}
\end{figure}
\clearpage
\subsection{Classification with Neural Network}
Moving on to the classification task, we once again faced the issue of an extreme number of possibilities to test in order to construct the optimal network. We limited our scope to only the optimizer Adam with $\rho_1 = 0.9, \rho_2 = 0.99$ and $\delta = 10^{-8}$ as before.

\subsubsection{First Attempt with Our Own Code}
Similar to the procedure in the creation of a network for numerical prediction, to make the optimal network for binary classification, we first employed a grid search of a fixed number of nodes per layer, and thereafter we altered the activation function on the hidden layers to see what gave the best results. In this process, we uncovered an error in the implementation. It looked as though the network predicted the average of the target vector, which gave an accuracy approximately equal to the frequency of target value 1 in the output array (since we are working with binary classification for this dataset). 

The source of the error, as well as its solution, remains elusive. We studied the implementation and behavior of our network extensively. Unit testing also passes for all of our methods\footnote{See our GitHub: \url{https://github.com/emmastorberg/FYS-STK4155_Project2}}.  

We initially scaled our data with \texttt{StandardScaler}, which we thought might be the issue, but changing the scaling to \texttt{MinMaxScaler} (also from Scikit-Learn \cite{sklearnScaling}) offered only an incremental change (from an accuracy of approximately 0.62 to 0.63). All gradient descent methods had also passed rigorous unit testing, and we also knew from the experiments in the previous section on the numerical prediction task that our neural network code was capable of producing reasonable results from a theoretical standpoint. 

Next, we tested our network on different datasets, including the iris dataset \cite{irisdataset}, which is a common and beginner-friendly dataset used for testing classification networks. 

Our implementation\footnote{We activate the output layer with another activation function better for multiclass classification, known as the \emph{softmax} function \cite{softmax}. Its definition is given in appendix A.} performs well on the iris data when scaled with \texttt{StandardScaler}, though not with \texttt{MinMaxScaler}. This showed that it was able to do some classification tasks, so we hoped the implementation was not the problem, but rather that the choice of parameters happened to lead us to a local minimum, which is why the accuracy stopped improving at an early iteration. However, an identical instantiation with PyTorch, an optimized tensor library for deep learning \cite{pytorch}, creates a network that is able to classify the data points without issue, so the instantiation and choice of parameters is most likely not the problem.

These confusing findings left us with an unclear path towards analyzing our code implementation on breast cancer data, as was the goal of this project. We found that through systematic testing and trial and error, we could achieve accuracy as high as 1 on the iris data when testing the whole dataset\footnote{There is a risk of overfitting in this case, but it goes to show that the neural network is still making sensible predictions, which it does not do at all with the breast cancer data.}. However, seeing as this is a multiclass rather than binary classification problem, continuing to work with a network implemented with our own code on the iris dataset will leave us unable to compare our results to a logistic regression, nor will we be exploring the application of machine learning methods in the medical field, as was our initial motivation.

Therefore we ultimately made the decision to continue working with the breast cancer data using PyTorch to explore the optimal instantiation of a network for such a dataset. The resulting neural network will also be compared to a logistic regression. 

\subsubsection{Continued Exploration with PyTorch}
Our search for the optimal set of hyperparameters for the task was in part systematic, and in part educated guesswork and trial and error. The first systematic test was a grid search, where we fixed the number of nodes in each layer. Throughout this task, we always used sigmoid as the activation function. A grid search table showing the best combination of number of layers and nodes per layer (with sigmoid activating all layers) is shown in figure \ref{fig:gridsearch_layers_nodes}. As we can see, the optimal result seems to be with fewer hidden layers, as overall one or two hidden layers performs the best. 

% our final activation function, and kept the activation functions of the hidden layers the same. Of our four options, having \textcolor{red}{(activation function)} activate all hidden layers resulted in highest accuracy overall. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/grid_search_layer_nodes.png}
    \caption{Grid search table showing the accuracy of neural networks with varying number of hidden layers, with a fixed number of nodes per layer and Leaky ReLU on all hidden layers, and sigmoid on the output layer.}
    \label{fig:gridsearch_layers_nodes}
\end{figure}

Using this as a starting point for further exploration, we continued looking into if a different set of activation functions for the hidden layers would have any effect, but Leaky ReLU performed well compared to the other activation functions, with only minimal improvement with use of ReLU. The plot of this can be found in appendix B, figure \ref{fig:activationfunctions_cost}. 

As the networks we tested had such high accuracy scores (for instance 98\% accuracy for a network with one hidden layer and 41 nodes in that layer, or 97\% with 51 nodes, as we see in figure \ref{fig:gridsearch_layers_nodes}), we consider this good for the time being. Values of our chosen neural network are shown in table \ref{tab:classificationtask}. However, though the network has high accuracy in its predictions, we must consider what type of errors it makes before determining this is a suitable network for diagnosing cancer. The confusion matrix is shown in figure \ref{fig:confusionmatrix}.

\begin{table}[h!]
  \centering
  \small
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Hyperparameter} & \textbf{Value} \\
    \hline
    Number of data points & $n = 569$ \\
    \hline
    Scaling of data & \texttt{StandardScaler} \\
    \hline
    Initial learning rate & $\eta = 0.1$ \\
    \hline
    Cost function & Cross-entropy \\
    \hline
    Input layer & 30 nodes, Leaky ReLU \\
    \hline
    Hidden layer & 51 nodes, Leaky ReLU \\
    \hline
    Output layer & 1 node, sigmoid \\
    \hline
  \end{tabular}
  \caption{Table showing values used in the best-performing neural network in classification task}
  \label{tab:classificationtask}
\end{table}
%\textcolor{magenta}{Figure: Activation functions. Plot the accuracy of the network (on cancer data) with different combinations of activation functions [leaky relu, relu, only sigmoid, softmax] (although I believe all should have sigmoid in the final layer), as a function of layers. (I think we should have a fixed number of nodes, unless we want to grid search this and choose the best)} \\ 
%\textcolor{purple}{Alternative figure to the one above. The same idea, but with the best layer and node size from the first heatmap/a grid search, as a function of epochs. This makes us able to visualize which activation function converges faster. I think this is a better plot} \\

%\textcolor{purple}{Figure: Accuracy for [SGD (with and without momentum), Adagrad (with and without momentum), RMSprop and Adam] (on the legend) as a function of epochs, with a fixed learning rate}
%We expect them to converge, some faster than others. With momentum should converge faster.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/plots/logreg.png}
    \caption{Confusion matrix for our choice of neural network instantiated with PyTorch}
    \label{fig:confusionmatrix}
\end{figure}

% INCLUDE THIS PARAGRAPH IF ANOTHER PLOT IS NECESSARY; OTHERWISE, MOVE TO DISCUSSION.

% We tuned our model again to lower FN rate. The new confusion matrix can be shown in figure \ref{fig:confusionmatrix2}.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/placeholders/confusionmatrix2.png}
%     \caption{\textcolor{purple}{Second confusion matrix for the neural net (on the cancer dataset, counting the number of TP, TN, FN and FP). Should definitely have less FN than FP this time around.}}
%     \label{fig:confusionmatrix2}
% \end{figure}

\subsection{Classification with Logistic Regression}
As discussed, an alternative to a neural network requiring far less tuning is logistic regression. This corresponds to a network with no hidden layers, activated by the sigmoid function. The resulting confusion matrix is shown in figure \ref{fig:logreg}. As we can see, the performance of logistic regression alone is worse compared to a neural network, though still acceptable. We view the higher false positive than false negative rate as a strength. However, we experienced how it requires much less time to set up and run, which should be taken into account when evaluating these methods.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/confusionmatrix.png}
    \caption{Confusion matrix from logistic regression}
    \label{fig:logreg}
\end{figure}

%\textcolor{magenta}{Should not be in report, but we should print accuracy/cf before and after scaling with the StandardScaler. Expect accuracy to increase. We can just cite on of our previous reports on scaling with StandardScaler}



