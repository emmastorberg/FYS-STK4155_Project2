This section documents the results of various experiments we did. Since the final aim of the
project is to predict breast cancer data accurately, we systematically tested all necessary components to perform this task, such that we ensure correctness (and reproducibility) of our final results. All plots show the performance of the model on testing data only.

\subsection{Linear Regression with Gradient Descent Methods}
In the first set of experiments, we are using a dataset in the form of a polynomial $f(x) = 5x^2 + 7x +3$ over the interval $x \in [0,1)$. We began with the most simple method, plain gradient descent, which we used with a constant learning rate. We tested the values $\eta \in \{ 0.1, 0.01, 0.001, 0.0001\}$ to find the best possible value to fixate for our experiments\footnote{The plots showing our testing can be found on GitHub in the directory \texttt{figures/all\_plots}.}, and based on this test, we saw that we had the best results when $\eta = 0.1$. With this learning rate, we studied convergence speed towards the minimum when using plain gradient descent with and with momentum. The plot of this (and the comparison to the analytical solution) is in figure \ref{fig:plainVSanalytical}, up to 10 epochs \cite{mediumEpochNumber}\cite{epochsBreastCancerArticle}.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/plain_mse_pr_iter_eta_1e-1.png}
    \caption{Plain gradient descent (PGD) with and without momentum compared to the analytical solution, with a constant learning rate of $\eta = 0.1$ and a momentum parameter $\gamma = 0.5$.}
    \label{fig:plainVSanalytical}
\end{figure}

Next, we looked at our stochastic methods, with and without momentum. In this case we tried a linearly decaying learning rate to see if we would have any major improvements over plain gradient descent. In this case, the linearly decaying learning rate used was:
\[
\eta_t = (1- \frac{t}{5})0.1 + \frac{t}{5} 0.001
\]
where we found the value $\eta_0 = 0.1$ after a few tests, as well as consulting the literature. We also kept the mini-batch size fixed at 4, which we found produced decent values in this case. The comparison of the MSE with and without momentum as a function of epochs is displayed in figure \ref{fig:sgdVSanalytical}. We also found our that when timed, stochastic methods gave us results quicker than the plain versions, with minimal detriment to the MSE. Therefore, from this point on, we will continue using our stochastic methods for computational efficiency.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/sgd_mse_pr_epoch_eta_1e-1.png}
    \caption{Stochastic gradient descent (SGD) with and without momentum compared to the analytical solution, with a constant learning rate of $\eta_0 = 0.1$ and a momentum parameter $\gamma = 0.5$. The size of the mini-batches is also fixed at 4.}
    \label{fig:sgdVSanalytical}
\end{figure}

Next, we looked at the methods with adaptive learning rates. All methods were initialized with the same global learning rate $\eta = 0.1$ to begin with, as this was the best result in both of our previous tests. We wanted to see how the different methods of adaptive learning rate converged, and if this behavior was consistent with the theory. The development of the MSE over epochs is shown in figure \ref{fig:MSEGD}. We see faster convergence in methods with momentum.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/placeholders/learningratesGD.png}
%     \caption{\textcolor{purple}{Plot of 3 subplots showing SGD w and w/o momentum for each of the three methods of adaptable learning rates.  Function of number of epochs. Learning rate on y-axis. See photo on Emma's phone.}}
%     \label{fig:learningratesGD}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/MSEGD.png}
    \caption{MSE as a function of epochs for the different emthods of adaptive learnign rate. For all methods, mini-batch size is fixed at 4, and momentum terms where applicable are scaled by $\gamma = 0.5$. SGD decays linearly, as in the previous figure.}
    \label{fig:MSEGD}
\end{figure}

As part of our testing process, we compared the performance of our gradients to that of the automatic differentiation library Autograd \cite{autograd}. Since our tests pass\footnote{Tests can be found on our GitHub: \url{https://github.com/emmastorberg/FYS-STK4155_Project2/blob/main/tests.py}}, we expected our implementations to give quite similar results as these pre-existing methods, which they did.
\vspace{3cm}

Up to this point, we have only used OLS. When we use ridge regression instead, there is the additional hyperparameter $\lambda$ to consider. We performed a grid search to determine what combination of $\lambda$ and gradient descent method to choose, and tested $\lambda \in \{10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}\}$ against five of the seven methods we tested for OLS in figure \ref{fig:MSEGD} (AdaGrad is excluded because of overall poor performance, as we also see in the figure \ref{fig:MSEGD}). The grid search table can be seen in figure \ref{fig:gridsearch_ridge}, where it is clear that smaller values of $\lambda$ do best, so we will use $\lambda = 0.00001$.

It is difficult to visually determine exactly which method is best, and upon reading off the values numerically, we also found minimal differences in the darkest regions. This is also consistent with the plots, where we see all methods except AdaGrad seem to converge nicely. Therefore, we determine that we will use Adam as we continue to use gradient descent in a neural network, as this seems in line with the literature on the subject.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/gridsearch_ridge.png}
    \caption{Grid search table of the seven possible methods compared for different values of the hyperparameter $\lambda$. In all cases, the global learning rate is set to 0.1, the mini-batch size is fixed at 4, and momentum terms are scaled by $\gamma = 0.5$.}
    \label{fig:gridsearch_ridge}
\end{figure}

\subsection{Numerical Prediction with Neural Network}
We saw that the optimal gradient descent methods for linear regression were RMSProp (with and without momentum) and Adam for OLS, and we chose Adam with $\lambda = 0.00001$ as the best-performing combination for ridge regression. 

There is an enormous number of possible neural networks we can create even with only the relatively few gradient descent methods and activation functions at our disposal, in addition to the limitation of computation time. We eliminated a few degrees of freedom here by sticking to Adam as our gradient descent method (based on its performance in the experiments up to this point), and with ReLU as the activation function for the outer layer, guided by the theoretical intuition of what values it can produce (only positive values; the range of our polynomial $f(x) = 5x^2 + 7x + 3$ is positive real numbers only).

To construct the network, we tried to sequentially eliminate degrees of freedom by fixating certain parameters, and thereafter exploring slight modifications to these. We started by using the same number of nodes per hidden layer, and performing a grid search to find the best combination. The results of this are shown in figure \ref{fig:gridsearch_numpred_layers_nodes}, with ReLU activating all hidden layers as well as the output layer. As we can see, there are many options that have low MSEs (dark color). We will choose a network with four nodes and four hidden layers for simplicity, which had an MSE of 0.09854. Also of note is that the lightest sections with high MSE predict zero everywhere, which is an unexpected finding.  
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/gridsearch_numpred_layers_nodes.png}
    \caption{Grid search table showing the MSE of neural networks with varying number of hidden layers, with a fixed number of nodes per layer and ReLU on all layers.}
    \label{fig:gridsearch_numpred_layers_nodes}
\end{figure}

Next, we tried varying what activation functions to use on the hidden layers. The plot of the resulting MSEs is shown in figure \ref{fig:activationfunctions_cost_numpred}. It looks like ReLU is (slightly) better for activating the hidden layers in this case, so we continued working with the same neural network with four hidden layers, four nodes per layer and ReLU activating everything. This became the basis for further modifications in hopes of improving performance. 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/all_plots/activationfunctions_cost_numpred.png}
    \caption{MSE of a neural network of four hidden layers and four nodes per layer activated by different functions in the hidden layers (ReLU was always used on the output layer).}
    \label{fig:activationfunctions_cost_numpred}
\end{figure}

With slight variations to the hidden layers, the number of nodes in them and what activation functions to use at each stage, we arrived at our best-performing network for this task. The hyperparameters used to create it can be seen in table \ref{tab:numericalprediction}: 
\begin{table}[h!]
  \centering
  \small
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Hyperparameter} & \textbf{Value} \\
    \hline
    Number of data points & $n = 100$ \\
    \hline
    Scaling of data & \texttt{StandardScaler} \\
    \hline
    Mini-batches number (epochs) & $M = 4$ \\
    \hline
    Mini-batch size & $20$ (test data) \\
    \hline
    Gradient descent method & Adam \\
    \hline
    Initial learning rate & $\eta = 0.1$ \\
    \hline
    Decay rate of first moment & $\rho_1 = 0.9$ \\
    \hline
    Decay rate of second moment & $\rho_2 = 0.99$ \\
    \hline
    Numerical stability constant & $\delta = 10^{-8}$ \\
    \hline
    Cost function & MSE \\
    \hline
    Input layer & 1 node, activation \\
    \hline
    Hidden layer 1 & 4 nodes, activation \\
    \hline
    Hidden layer 2 & 4 nodes, activation \\
    \hline
    Hidden layer 3 & 4 nodes, activation \\
    \hline
    Hidden layer 4 & 4 nodes, activation \\
    \hline
    Output layer & 1 node, ReLU \\
    \hline
  \end{tabular}
  \caption{The chosen hyperparameters for best performance in numerical prediction task.}
  \label{tab:numericalprediction}
\end{table}

The neural network described \textcolor{blue}{above} was able to predict with an MSE of \textcolor{red}{X} on the testing data. The MSE of our best-performing neural network, OLS and ridge regression compared to stochastic gradient descent using Scikit-Learn can be seen in figure \ref{fig:numericalprediction}. We also show the predictions made by each method in figure \ref{fig:predictioncomparison}.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/placeholders/numericalprediction.png}
    \caption{MSE as a function of epochs for OLS, Ridge and neural network (our best version of all three methods using Adam to optimize), also compared to Scikit-Learn ridge regression by SGD with $\eta = 0.1$.}
    \label{fig:numericalprediction}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/placeholders/predictioncomparison.png}
    \caption{Comparison of predictions with optimal versions of all methods shown.}
    \label{fig:predictioncomparison}
\end{figure}
As we can see, it seems that the best method overall for this task is \textcolor{red}{(method)} with parameters \textcolor{red}{(parameters)}. 

\subsection{Classification with Neural Network}
Moving on to the classification task, we once again faced the issue of an extreme number of possibilities to test in order to construct the best-performing network. We limited our scope to only \textcolor{red}{(method)} with parameters \textcolor{red}{(parameters)}.

\subsubsection{First Attempt with Our Own Implementation}
Similar to the procedure in the creation of a network for numerical prediction, to make the optimal network for binary classification, we first employed a grid search of a fixed number of nodes per layer, and thereafter we altered the activation function on the hidden layers to see what gave the best results. In this process, we uncovered an error in the implementation. It looked as though the network predicted the average of the target vector, which gave an accuracy approximately equal to the frequency of target value 1 in the output array (since we are working with binary classification for this dataset). 

The source of the error, as well as its solution, remains elusive. We studied the implementation and behavior of our network extensively. Unit testing also passes for all of our methods\footnote{See our GitHub: \url{https://github.com/emmastorberg/FYS-STK4155_Project2}}.  

We initially scaled our data with \texttt{StandardScaler}, which we thought might be the issue, but changing the scaling to \texttt{MinMaxScaler} (also from Scikit-Learn \cite{sklearnScaling}) offered only an incremental change (from an accuracy of approximately 0.62 to 0.63). All gradient descent methods had also passed rigorous unit testing, and we also knew from the experiments in the previous section on the numerical prediction task that our neural network code was capable of producing reasonable results from a theoretical standpoint. 

We tested our network on different datasets, including the iris dataset \cite{irisdataset}, which is a common and beginner-friendly dataset used for testing classification networks. Our implementation\footnote{We activate the output layer with another activation function better for multiclass classification, known as the \emph{softmax} function \cite{softmax}. Its definition is given in appendix A.} performs well on this dataset when scaled with \texttt{StandardScaler}, though not with \texttt{MinMaxScaler}. This showed that it was able to do some classification tasks, so we hoped the implementation was not the problem, but rather that the choice of parameters happened to lead us to a local minimum, which is why the accuracy stopped improving at an early iteration. However, an identical instantiation with PyTorch, an optimized tensor library for deep learning \cite{pytorch}, creates a network that is able to classify the data points without issue, so the instantiation and choice of parameters is most likely not the problem.

These confusing findings left us with an unclear path towards analyzing our code implementation on breast cancer data, as was the goal of this project. We found that through systematic testing and trial and error, we could achieve accuracy as high as \textcolor{red}{(accuracy)}\% on the iris data. However, seeing as this is a multiclass rather than binary classification problem, continuing to work with a network implemented with our own code on the iris dataset will leave us unable to compare our results to a logistic regression, nor will we be exploring the application of machine learning methods in the medical field, as was our initial motivation.

Therefore we ultimately made the decision to continue working with the breast cancer data using PyTorch to explore the optimal instantiation of a network for such a dataset. The resulting neural network will also be compared to a logistic regression. 

\subsubsection{Continued Exploration with PyTorch}
Our search for the optimal set of hyperparameters for the task was in part systematic, and in part educated guesswork and trial and error. The first systematic test was a grid search, where we fixed the number of nodes in each layer. Throughout this task, we always used sigmoid as the activation function. A grid search table showing the best combination of number of layers and nodes per layer (with sigmoid activating all layers) is shown in figure \ref{fig:gridsearch_layers_nodes}. 

% our final activation function, and kept the activation functions of the hidden layers the same. Of our four options, having \textcolor{red}{(activation function)} activate all hidden layers resulted in highest accuracy overall. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/placeholders/gridsearch_layers_nodes.png}
    \caption{\textcolor{purple}{Heat map showing the accuracy for the neural net (on the cancer dataset), as a function of hidden layers (1-5) and nodes in each hidden layer (1-10)}}
    \label{fig:gridsearch_layers_nodes}
\end{figure}

As we can see, the optimal result seems to be \textcolor{red}{(nodes in each layer explained here)}. Using this as a starting point for further exploration, we continued looking into if a different set of activation functions for the hidden layers would have any effect. In figure \ref{fig:activationfunctions_cost}, we see all four options for activation functions on a neural network with \textcolor{red}{(nodes in each layer explained here)}, as determined in figure \ref{fig:gridsearch_layers_nodes}. We see that \textcolor{red}{(activation function)} works \textcolor{red}{better/worse} than our initial test using sigmoid everywhere, so we continued tweaking a network in the form of \textcolor{red}{best finding here}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/placeholders/activationfunctions_cost.png}
    \caption{\textcolor{purple}{Cost as a function of number of epochs, testing for four different neural networks. They all have the size determined in the grid search, but now we want to see if changing the activation of the hidden layers gives us any improvements.}}
    \label{fig:activationfunctions_cost}
\end{figure}

Through less organized trial and error testing, we began with the optimal network so far, with size \textcolor{red}{(layer and node info)} and \textcolor{red}{(activation function)} on all hidden layers. We were able to find that with \textcolor{red}{(activation function on hidden layers)} and \textcolor{red}{(alternative neural network structure)}, we could now achieve an accuracy of up to \textcolor{red}{(accuracy)}\% for the Wisconsin Breast Cancer dataset. \textcolor{green}{We expected from the theory that ReLU would do well, and in particular, offer faster convergence than the other methods. This applies/does not apply in this case as well.} The full set of chosen hyperparameters for this neural network are shown in table \ref{tab:classificationtask}.

\begin{table}[h!]
  \centering
  \small
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Hyperparameter} & \textbf{Value} \\
    \hline
    Number of data points & $n = $ \\
    \hline
    Scaling of data & \texttt{StandardScaler} \\
    \hline
    Number of mini-batches (epochs) & $M =$ \\
    \hline
    Mini-batch size & $\frac{n}{M} = $ \\
    \hline
    Gradient descent method & Adam \\
    \hline
    Initial learning rate & $\eta = $ \\
    \hline
    Decay rate of first moment & $\rho_1 =$ \\
    \hline
    Decay rate of second moment & $\rho_2 = $ \\
    \hline
    Numerical stability constant & $\delta = $ \\
    \hline
    Cost function & Cross-entropy \\
    \hline
    Input layer & Size, activation \\
    \hline
    Layer 2 & Size, activation \\
    \hline
    Layer 3 & Size, activation \\
    \hline
    Output layer & Size, activation \\
    \hline
  \end{tabular}
  \caption{Table showing hyperparameters chosen for best-performing neural network in classification task}
  \label{tab:classificationtask}
\end{table}
%\textcolor{magenta}{Figure: Activation functions. Plot the accuracy of the network (on cancer data) with different combinations of activation functions [leaky relu, relu, only sigmoid, softmax] (although I believe all should have sigmoid in the final layer), as a function of layers. (I think we should have a fixed number of nodes, unless we want to grid search this and choose the best)} \\ 
%\textcolor{purple}{Alternative figure to the one above. The same idea, but with the best layer and node size from the first heatmap/a grid search, as a function of epochs. This makes us able to visualize which activation function converges faster. I think this is a better plot} \\

%\textcolor{purple}{Figure: Accuracy for [SGD (with and without momentum), Adagrad (with and without momentum), RMSprop and Adam] (on the legend) as a function of epochs, with a fixed learning rate}
%We expect them to converge, some faster than others. With momentum should converge faster.

The network as described \textcolor{blue}{above} has \textcolor{red}{(accuracy)}\% accuracy in its predictions overall. However, we must also consider what type of inaccuracy is most prevalent before determining this is the best network for the task. False positive and negative rate can be seen in the confusion matrix in figure \ref{fig:confusionmatrix}.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/placeholders/confusionmatrix.png}
    \caption{\textcolor{purple}{Confusion matrix for the neural net (on the cancer dataset, counting the number of TP, TN, FN and FP)}}
    \label{fig:confusionmatrix}
\end{figure}

% INCLUDE THIS PARAGRAPH IF ANOTHER PLOT IS NECESSARY; OTHERWISE, MOVE TO DISCUSSION.

% We tuned our model again to lower FN rate. The new confusion matrix can be shown in figure \ref{fig:confusionmatrix2}.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/placeholders/confusionmatrix2.png}
%     \caption{\textcolor{purple}{Second confusion matrix for the neural net (on the cancer dataset, counting the number of TP, TN, FN and FP). Should definitely have less FN than FP this time around.}}
%     \label{fig:confusionmatrix2}
% \end{figure}

\subsection{Classification with Logistic Regression}
As discussed, an alternative to a neural network requiring far less tuning is logistic regression. This corresponds to a network with no hidden layers, activated by the sigmoid function. The resulting confusion matrix is shown in figure \ref{fig:logreg}. As we can see, the performance of logistic regression alone is relatively poor compared to a neural network. However, we experienced how it requires much less time to set up and run, which should be taken into account when evaluating these methods.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/plots/logreg.png}
    \caption{Confusion matrix from logistic regression}
    \label{fig:logreg}
\end{figure}

%\textcolor{magenta}{Should not be in report, but we should print accuracy/cf before and after scaling with the StandardScaler. Expect accuracy to increase. We can just cite on of our previous reports on scaling with StandardScaler}



