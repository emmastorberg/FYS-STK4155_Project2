\begin{adjustwidth}{2cm}{2cm} % Adjust the margins: {left margin}{right margin}
        Neural networks are more trendy than ever! In this project, we are diving into gradient descent as a standalone method for determining optimal parameters in linear regression, as well as from the perspective of being a key component in neural networks. We implement it, compare it with methods that use analytical expressions for optimal parameters, and try to make predictions ourselves on the dataset \textcolor{red}{(add dataset name here)} from \textcolor{red}{(add dataset source here)} \cite{franke} \textcolor{red}{(citing Franke for technical BibLatex reasons)}. We make use of different activation functions. We also explore the classification capabilities of neural networks, and compare its performance in this area to that of logsitic regression. \textcolor{red}{(Add a sentence or two about results here.)} The key takeaway is that \textcolor{red}{(main point we want to highlight here)}.
      \end{adjustwidth}

% PROJECT 1 ABSTRACT FOR COMPARISON:
% Understanding the intricacies of machine learning is an essential part of navigating our data-driven world. In this project, we explored various methods for solving linear regression on the 2D Franke function \cite{franke} and a dataset from the United States Geological Survey \cite{usgovterraindata}. Our analysis focused on Ordinary Least Squares (OLS), ridge regression and LASSO regression, evaluated through statistical metrics such as the mean squared error and R$^2$ score. Additionally, we used resampling methods like bootstrap resampling and $k$-fold cross-validation to achieve more reliable results. We investigated the bias-variance trade-off and challenged our models with the goal of first inducing overfitting, and thereafter combating it using our understanding of the theory and techniques we have learned. Initially, our OLS model performed well, but the introduction of regularization terms revealed an error in our procedure for determining optimal hyperparameters, resulting in unreasonable values despite passing tests against the methods of \lstinline[basicstyle=\small\ttfamily]|scikit-learn|. We emphasize the importance of following sound statistical intuition and employing supplementary methods such as visual inspection when interpreting results, rather than solely relying on numerical metrics.